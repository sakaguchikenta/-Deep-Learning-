{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.48618104  0.21551717 -0.57530677]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([[1,0,0,0,0,0,0,]])\n",
    "w = np.random.randn(7,3)\n",
    "h = np.dot(c,w)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "c0 = np.array([[1,0,0,0,0,0,0,]])\n",
    "c1 = np.array([[0,0,1,0,0,0,0]])\n",
    "\n",
    "W_in = np.random.randn(7,3)\n",
    "W_out = np.random.randn(3,7)\n",
    "\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "def create_contexts_target(corpus, window_size =1):\n",
    "    target = corpus[window_size: -window_size]\n",
    "    contexts = []\n",
    "    \n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size +1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "        \n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size =1)\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V,H = vocab_size, hidden_size\n",
    "        \n",
    "        W_in = 0.01 * np.random.randn(V,H).astype('f')\n",
    "        W_out = 0.01*np.random.randn(h,V).astype('f')\n",
    "        \n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        self.word_vecs =W_in\n",
    "        \n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:,0])\n",
    "        h1 = self.in_layer1.forward(contexts[:,1])\n",
    "        h = (h0 + h1)*0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss  = self.loss_layer.forward(score,target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simple_cbow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-16911e6dd62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimple_cbow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleCBOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_contexts_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_one_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simple_cbow'"
     ]
    }
   ],
   "source": [
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam \n",
    "from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vacab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ch4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self,W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "    def forward(self, idx):\n",
    "            W, = self.params\n",
    "            self.idx = idx\n",
    "            out = W[idx]\n",
    "            return out\n",
    "    def backward(self, dout):\n",
    "            dw, = self.grads\n",
    "            dw[...] = 0\n",
    "           \n",
    "            for i , word_id in enumerate(self.idx):\n",
    "                dw[word_id] += dout[i]\n",
    "            \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "    def forward(self, h ,idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0],1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(courpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size +1)]\n",
    "        self.emped_dot_layers = [EmbeddingDot(W) for _ in range(sample_size +1)]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_nagetive_sample(target)\n",
    "        \n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:,i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_laers[1 + i].forward(score, negative_label)\n",
    "        return loss   \n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "            dh = 0\n",
    "            for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "                dscore = l0.backward(dout)\n",
    "                dh += l1.backward(dscore)\n",
    "            return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Embedding\n",
    "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H  = vocab_size, hidden_size\n",
    "        \n",
    "        W_in = 0.01 * np.random.randn(V,H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        \n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out,corpus, power=0.75, sample_size=5)\n",
    "        \n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        self.word_vecs =W_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, contexts, target):\n",
    "    h = 0\n",
    "    for i, layer in enumerate(self.in_layers):\n",
    "        h += layer.forward(contexts[:, i])\n",
    "    h *= 1 /len(self.in_layers)\n",
    "    loss = self.ns_loss.forward(h,target)\n",
    "    return loss\n",
    "\n",
    "def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout*= 1/ len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "              layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "| epoch 1 |  iter 1 / 9295 | time 0[s] | loss 4.16\n",
      "| epoch 1 |  iter 21 / 9295 | time 0[s] | loss 4.16\n",
      "| epoch 1 |  iter 41 / 9295 | time 1[s] | loss 4.15\n",
      "| epoch 1 |  iter 61 / 9295 | time 2[s] | loss 4.12\n",
      "| epoch 1 |  iter 81 / 9295 | time 2[s] | loss 4.05\n",
      "| epoch 1 |  iter 101 / 9295 | time 3[s] | loss 3.93\n",
      "| epoch 1 |  iter 121 / 9295 | time 3[s] | loss 3.78\n",
      "| epoch 1 |  iter 141 / 9295 | time 4[s] | loss 3.63\n",
      "| epoch 1 |  iter 161 / 9295 | time 5[s] | loss 3.48\n",
      "| epoch 1 |  iter 181 / 9295 | time 5[s] | loss 3.36\n",
      "| epoch 1 |  iter 201 / 9295 | time 6[s] | loss 3.26\n",
      "| epoch 1 |  iter 221 / 9295 | time 7[s] | loss 3.14\n",
      "| epoch 1 |  iter 241 / 9295 | time 7[s] | loss 3.08\n",
      "| epoch 1 |  iter 261 / 9295 | time 8[s] | loss 3.00\n",
      "| epoch 1 |  iter 281 / 9295 | time 8[s] | loss 2.95\n",
      "| epoch 1 |  iter 301 / 9295 | time 9[s] | loss 2.93\n",
      "| epoch 1 |  iter 321 / 9295 | time 10[s] | loss 2.87\n",
      "| epoch 1 |  iter 341 / 9295 | time 10[s] | loss 2.85\n",
      "| epoch 1 |  iter 361 / 9295 | time 11[s] | loss 2.81\n",
      "| epoch 1 |  iter 381 / 9295 | time 12[s] | loss 2.78\n",
      "| epoch 1 |  iter 401 / 9295 | time 12[s] | loss 2.75\n",
      "| epoch 1 |  iter 421 / 9295 | time 13[s] | loss 2.75\n",
      "| epoch 1 |  iter 441 / 9295 | time 14[s] | loss 2.72\n",
      "| epoch 1 |  iter 461 / 9295 | time 14[s] | loss 2.71\n",
      "| epoch 1 |  iter 481 / 9295 | time 15[s] | loss 2.69\n",
      "| epoch 1 |  iter 501 / 9295 | time 15[s] | loss 2.67\n",
      "| epoch 1 |  iter 521 / 9295 | time 16[s] | loss 2.68\n",
      "| epoch 1 |  iter 541 / 9295 | time 17[s] | loss 2.65\n",
      "| epoch 1 |  iter 561 / 9295 | time 17[s] | loss 2.68\n",
      "| epoch 1 |  iter 581 / 9295 | time 18[s] | loss 2.66\n",
      "| epoch 1 |  iter 601 / 9295 | time 19[s] | loss 2.66\n",
      "| epoch 1 |  iter 621 / 9295 | time 19[s] | loss 2.64\n",
      "| epoch 1 |  iter 641 / 9295 | time 20[s] | loss 2.66\n",
      "| epoch 1 |  iter 661 / 9295 | time 20[s] | loss 2.63\n",
      "| epoch 1 |  iter 681 / 9295 | time 21[s] | loss 2.61\n",
      "| epoch 1 |  iter 701 / 9295 | time 22[s] | loss 2.61\n",
      "| epoch 1 |  iter 721 / 9295 | time 22[s] | loss 2.61\n",
      "| epoch 1 |  iter 741 / 9295 | time 23[s] | loss 2.62\n",
      "| epoch 1 |  iter 761 / 9295 | time 24[s] | loss 2.59\n",
      "| epoch 1 |  iter 781 / 9295 | time 24[s] | loss 2.59\n",
      "| epoch 1 |  iter 801 / 9295 | time 25[s] | loss 2.61\n",
      "| epoch 1 |  iter 821 / 9295 | time 25[s] | loss 2.56\n",
      "| epoch 1 |  iter 841 / 9295 | time 26[s] | loss 2.59\n",
      "| epoch 1 |  iter 861 / 9295 | time 27[s] | loss 2.58\n",
      "| epoch 1 |  iter 881 / 9295 | time 27[s] | loss 2.58\n",
      "| epoch 1 |  iter 901 / 9295 | time 28[s] | loss 2.60\n",
      "| epoch 1 |  iter 921 / 9295 | time 29[s] | loss 2.56\n",
      "| epoch 1 |  iter 941 / 9295 | time 29[s] | loss 2.58\n",
      "| epoch 1 |  iter 961 / 9295 | time 30[s] | loss 2.56\n",
      "| epoch 1 |  iter 981 / 9295 | time 31[s] | loss 2.56\n",
      "| epoch 1 |  iter 1001 / 9295 | time 31[s] | loss 2.56\n",
      "| epoch 1 |  iter 1021 / 9295 | time 32[s] | loss 2.57\n",
      "| epoch 1 |  iter 1041 / 9295 | time 32[s] | loss 2.54\n",
      "| epoch 1 |  iter 1061 / 9295 | time 33[s] | loss 2.56\n",
      "| epoch 1 |  iter 1081 / 9295 | time 34[s] | loss 2.53\n",
      "| epoch 1 |  iter 1101 / 9295 | time 34[s] | loss 2.55\n",
      "| epoch 1 |  iter 1121 / 9295 | time 35[s] | loss 2.55\n",
      "| epoch 1 |  iter 1141 / 9295 | time 36[s] | loss 2.53\n",
      "| epoch 1 |  iter 1161 / 9295 | time 36[s] | loss 2.58\n",
      "| epoch 1 |  iter 1181 / 9295 | time 37[s] | loss 2.54\n",
      "| epoch 1 |  iter 1201 / 9295 | time 37[s] | loss 2.54\n",
      "| epoch 1 |  iter 1221 / 9295 | time 38[s] | loss 2.55\n",
      "| epoch 1 |  iter 1241 / 9295 | time 39[s] | loss 2.52\n",
      "| epoch 1 |  iter 1261 / 9295 | time 39[s] | loss 2.53\n",
      "| epoch 1 |  iter 1281 / 9295 | time 40[s] | loss 2.54\n",
      "| epoch 1 |  iter 1301 / 9295 | time 41[s] | loss 2.52\n",
      "| epoch 1 |  iter 1321 / 9295 | time 41[s] | loss 2.56\n",
      "| epoch 1 |  iter 1341 / 9295 | time 42[s] | loss 2.55\n",
      "| epoch 1 |  iter 1361 / 9295 | time 43[s] | loss 2.52\n",
      "| epoch 1 |  iter 1381 / 9295 | time 43[s] | loss 2.53\n",
      "| epoch 1 |  iter 1401 / 9295 | time 44[s] | loss 2.53\n",
      "| epoch 1 |  iter 1421 / 9295 | time 44[s] | loss 2.52\n",
      "| epoch 1 |  iter 1441 / 9295 | time 45[s] | loss 2.53\n",
      "| epoch 1 |  iter 1461 / 9295 | time 46[s] | loss 2.51\n",
      "| epoch 1 |  iter 1481 / 9295 | time 46[s] | loss 2.53\n",
      "| epoch 1 |  iter 1501 / 9295 | time 47[s] | loss 2.48\n",
      "| epoch 1 |  iter 1521 / 9295 | time 48[s] | loss 2.51\n",
      "| epoch 1 |  iter 1541 / 9295 | time 48[s] | loss 2.50\n",
      "| epoch 1 |  iter 1561 / 9295 | time 49[s] | loss 2.51\n",
      "| epoch 1 |  iter 1581 / 9295 | time 49[s] | loss 2.52\n",
      "| epoch 1 |  iter 1601 / 9295 | time 50[s] | loss 2.51\n",
      "| epoch 1 |  iter 1621 / 9295 | time 51[s] | loss 2.51\n",
      "| epoch 1 |  iter 1641 / 9295 | time 51[s] | loss 2.49\n",
      "| epoch 1 |  iter 1661 / 9295 | time 52[s] | loss 2.53\n",
      "| epoch 1 |  iter 1681 / 9295 | time 53[s] | loss 2.53\n",
      "| epoch 1 |  iter 1701 / 9295 | time 53[s] | loss 2.51\n",
      "| epoch 1 |  iter 1721 / 9295 | time 54[s] | loss 2.48\n",
      "| epoch 1 |  iter 1741 / 9295 | time 55[s] | loss 2.52\n",
      "| epoch 1 |  iter 1761 / 9295 | time 55[s] | loss 2.47\n",
      "| epoch 1 |  iter 1781 / 9295 | time 56[s] | loss 2.50\n",
      "| epoch 1 |  iter 1801 / 9295 | time 57[s] | loss 2.52\n",
      "| epoch 1 |  iter 1821 / 9295 | time 57[s] | loss 2.50\n",
      "| epoch 1 |  iter 1841 / 9295 | time 58[s] | loss 2.50\n",
      "| epoch 1 |  iter 1861 / 9295 | time 58[s] | loss 2.52\n",
      "| epoch 1 |  iter 1881 / 9295 | time 59[s] | loss 2.50\n",
      "| epoch 1 |  iter 1901 / 9295 | time 60[s] | loss 2.50\n",
      "| epoch 1 |  iter 1921 / 9295 | time 60[s] | loss 2.48\n",
      "| epoch 1 |  iter 1941 / 9295 | time 61[s] | loss 2.50\n",
      "| epoch 1 |  iter 1961 / 9295 | time 62[s] | loss 2.49\n",
      "| epoch 1 |  iter 1981 / 9295 | time 62[s] | loss 2.49\n",
      "| epoch 1 |  iter 2001 / 9295 | time 63[s] | loss 2.51\n",
      "| epoch 1 |  iter 2021 / 9295 | time 63[s] | loss 2.50\n",
      "| epoch 1 |  iter 2041 / 9295 | time 64[s] | loss 2.48\n",
      "| epoch 1 |  iter 2061 / 9295 | time 65[s] | loss 2.46\n",
      "| epoch 1 |  iter 2081 / 9295 | time 65[s] | loss 2.47\n",
      "| epoch 1 |  iter 2101 / 9295 | time 66[s] | loss 2.47\n",
      "| epoch 1 |  iter 2121 / 9295 | time 67[s] | loss 2.48\n",
      "| epoch 1 |  iter 2141 / 9295 | time 67[s] | loss 2.48\n",
      "| epoch 1 |  iter 2161 / 9295 | time 68[s] | loss 2.45\n",
      "| epoch 1 |  iter 2181 / 9295 | time 69[s] | loss 2.50\n",
      "| epoch 1 |  iter 2201 / 9295 | time 69[s] | loss 2.49\n",
      "| epoch 1 |  iter 2221 / 9295 | time 70[s] | loss 2.50\n",
      "| epoch 1 |  iter 2241 / 9295 | time 70[s] | loss 2.51\n",
      "| epoch 1 |  iter 2261 / 9295 | time 71[s] | loss 2.49\n",
      "| epoch 1 |  iter 2281 / 9295 | time 72[s] | loss 2.49\n",
      "| epoch 1 |  iter 2301 / 9295 | time 72[s] | loss 2.46\n",
      "| epoch 1 |  iter 2321 / 9295 | time 73[s] | loss 2.49\n",
      "| epoch 1 |  iter 2341 / 9295 | time 74[s] | loss 2.49\n",
      "| epoch 1 |  iter 2361 / 9295 | time 74[s] | loss 2.47\n",
      "| epoch 1 |  iter 2381 / 9295 | time 75[s] | loss 2.50\n",
      "| epoch 1 |  iter 2401 / 9295 | time 76[s] | loss 2.50\n",
      "| epoch 1 |  iter 2421 / 9295 | time 76[s] | loss 2.46\n",
      "| epoch 1 |  iter 2441 / 9295 | time 77[s] | loss 2.49\n",
      "| epoch 1 |  iter 2461 / 9295 | time 77[s] | loss 2.49\n",
      "| epoch 1 |  iter 2481 / 9295 | time 78[s] | loss 2.46\n",
      "| epoch 1 |  iter 2501 / 9295 | time 79[s] | loss 2.47\n",
      "| epoch 1 |  iter 2521 / 9295 | time 79[s] | loss 2.48\n",
      "| epoch 1 |  iter 2541 / 9295 | time 80[s] | loss 2.46\n",
      "| epoch 1 |  iter 2561 / 9295 | time 81[s] | loss 2.47\n",
      "| epoch 1 |  iter 2581 / 9295 | time 81[s] | loss 2.46\n",
      "| epoch 1 |  iter 2601 / 9295 | time 82[s] | loss 2.48\n",
      "| epoch 1 |  iter 2621 / 9295 | time 82[s] | loss 2.45\n",
      "| epoch 1 |  iter 2641 / 9295 | time 83[s] | loss 2.47\n",
      "| epoch 1 |  iter 2661 / 9295 | time 84[s] | loss 2.44\n",
      "| epoch 1 |  iter 2681 / 9295 | time 84[s] | loss 2.46\n",
      "| epoch 1 |  iter 2701 / 9295 | time 85[s] | loss 2.48\n",
      "| epoch 1 |  iter 2721 / 9295 | time 86[s] | loss 2.46\n",
      "| epoch 1 |  iter 2741 / 9295 | time 86[s] | loss 2.46\n",
      "| epoch 1 |  iter 2761 / 9295 | time 87[s] | loss 2.49\n",
      "| epoch 1 |  iter 2781 / 9295 | time 88[s] | loss 2.45\n",
      "| epoch 1 |  iter 2801 / 9295 | time 88[s] | loss 2.43\n",
      "| epoch 1 |  iter 2821 / 9295 | time 89[s] | loss 2.46\n",
      "| epoch 1 |  iter 2841 / 9295 | time 89[s] | loss 2.46\n",
      "| epoch 1 |  iter 2861 / 9295 | time 90[s] | loss 2.48\n",
      "| epoch 1 |  iter 2881 / 9295 | time 91[s] | loss 2.42\n",
      "| epoch 1 |  iter 2901 / 9295 | time 91[s] | loss 2.47\n",
      "| epoch 1 |  iter 2921 / 9295 | time 92[s] | loss 2.47\n",
      "| epoch 1 |  iter 2941 / 9295 | time 93[s] | loss 2.47\n",
      "| epoch 1 |  iter 2961 / 9295 | time 93[s] | loss 2.46\n",
      "| epoch 1 |  iter 2981 / 9295 | time 94[s] | loss 2.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 3001 / 9295 | time 94[s] | loss 2.46\n",
      "| epoch 1 |  iter 3021 / 9295 | time 95[s] | loss 2.47\n",
      "| epoch 1 |  iter 3041 / 9295 | time 96[s] | loss 2.44\n",
      "| epoch 1 |  iter 3061 / 9295 | time 96[s] | loss 2.45\n",
      "| epoch 1 |  iter 3081 / 9295 | time 97[s] | loss 2.45\n",
      "| epoch 1 |  iter 3101 / 9295 | time 98[s] | loss 2.40\n",
      "| epoch 1 |  iter 3121 / 9295 | time 98[s] | loss 2.41\n",
      "| epoch 1 |  iter 3141 / 9295 | time 99[s] | loss 2.42\n",
      "| epoch 1 |  iter 3161 / 9295 | time 100[s] | loss 2.45\n",
      "| epoch 1 |  iter 3181 / 9295 | time 100[s] | loss 2.44\n",
      "| epoch 1 |  iter 3201 / 9295 | time 101[s] | loss 2.42\n",
      "| epoch 1 |  iter 3221 / 9295 | time 101[s] | loss 2.41\n",
      "| epoch 1 |  iter 3241 / 9295 | time 102[s] | loss 2.45\n",
      "| epoch 1 |  iter 3261 / 9295 | time 103[s] | loss 2.41\n",
      "| epoch 1 |  iter 3281 / 9295 | time 103[s] | loss 2.45\n",
      "| epoch 1 |  iter 3301 / 9295 | time 104[s] | loss 2.43\n",
      "| epoch 1 |  iter 3321 / 9295 | time 105[s] | loss 2.46\n",
      "| epoch 1 |  iter 3341 / 9295 | time 105[s] | loss 2.44\n",
      "| epoch 1 |  iter 3361 / 9295 | time 106[s] | loss 2.42\n",
      "| epoch 1 |  iter 3381 / 9295 | time 106[s] | loss 2.45\n",
      "| epoch 1 |  iter 3401 / 9295 | time 107[s] | loss 2.41\n",
      "| epoch 1 |  iter 3421 / 9295 | time 108[s] | loss 2.42\n",
      "| epoch 1 |  iter 3441 / 9295 | time 108[s] | loss 2.42\n",
      "| epoch 1 |  iter 3461 / 9295 | time 109[s] | loss 2.42\n",
      "| epoch 1 |  iter 3481 / 9295 | time 110[s] | loss 2.44\n",
      "| epoch 1 |  iter 3501 / 9295 | time 110[s] | loss 2.43\n",
      "| epoch 1 |  iter 3521 / 9295 | time 111[s] | loss 2.43\n",
      "| epoch 1 |  iter 3541 / 9295 | time 112[s] | loss 2.41\n",
      "| epoch 1 |  iter 3561 / 9295 | time 112[s] | loss 2.43\n",
      "| epoch 1 |  iter 3581 / 9295 | time 113[s] | loss 2.41\n",
      "| epoch 1 |  iter 3601 / 9295 | time 113[s] | loss 2.42\n",
      "| epoch 1 |  iter 3621 / 9295 | time 114[s] | loss 2.43\n",
      "| epoch 1 |  iter 3641 / 9295 | time 115[s] | loss 2.44\n",
      "| epoch 1 |  iter 3661 / 9295 | time 115[s] | loss 2.41\n",
      "| epoch 1 |  iter 3681 / 9295 | time 116[s] | loss 2.45\n",
      "| epoch 1 |  iter 3701 / 9295 | time 117[s] | loss 2.42\n",
      "| epoch 1 |  iter 3721 / 9295 | time 117[s] | loss 2.42\n",
      "| epoch 1 |  iter 3741 / 9295 | time 118[s] | loss 2.42\n",
      "| epoch 1 |  iter 3761 / 9295 | time 119[s] | loss 2.42\n",
      "| epoch 1 |  iter 3781 / 9295 | time 119[s] | loss 2.41\n",
      "| epoch 1 |  iter 3801 / 9295 | time 120[s] | loss 2.42\n",
      "| epoch 1 |  iter 3821 / 9295 | time 120[s] | loss 2.37\n",
      "| epoch 1 |  iter 3841 / 9295 | time 121[s] | loss 2.41\n",
      "| epoch 1 |  iter 3861 / 9295 | time 122[s] | loss 2.40\n",
      "| epoch 1 |  iter 3881 / 9295 | time 122[s] | loss 2.41\n",
      "| epoch 1 |  iter 3901 / 9295 | time 123[s] | loss 2.43\n",
      "| epoch 1 |  iter 3921 / 9295 | time 124[s] | loss 2.40\n",
      "| epoch 1 |  iter 3941 / 9295 | time 124[s] | loss 2.42\n",
      "| epoch 1 |  iter 3961 / 9295 | time 125[s] | loss 2.38\n",
      "| epoch 1 |  iter 3981 / 9295 | time 125[s] | loss 2.39\n",
      "| epoch 1 |  iter 4001 / 9295 | time 126[s] | loss 2.42\n",
      "| epoch 1 |  iter 4021 / 9295 | time 127[s] | loss 2.40\n",
      "| epoch 1 |  iter 4041 / 9295 | time 127[s] | loss 2.40\n",
      "| epoch 1 |  iter 4061 / 9295 | time 128[s] | loss 2.39\n",
      "| epoch 1 |  iter 4081 / 9295 | time 129[s] | loss 2.38\n",
      "| epoch 1 |  iter 4101 / 9295 | time 129[s] | loss 2.38\n",
      "| epoch 1 |  iter 4121 / 9295 | time 130[s] | loss 2.37\n",
      "| epoch 1 |  iter 4141 / 9295 | time 131[s] | loss 2.41\n",
      "| epoch 1 |  iter 4161 / 9295 | time 131[s] | loss 2.42\n",
      "| epoch 1 |  iter 4181 / 9295 | time 132[s] | loss 2.42\n",
      "| epoch 1 |  iter 4201 / 9295 | time 132[s] | loss 2.37\n",
      "| epoch 1 |  iter 4221 / 9295 | time 133[s] | loss 2.39\n",
      "| epoch 1 |  iter 4241 / 9295 | time 134[s] | loss 2.40\n",
      "| epoch 1 |  iter 4261 / 9295 | time 134[s] | loss 2.39\n",
      "| epoch 1 |  iter 4281 / 9295 | time 135[s] | loss 2.38\n",
      "| epoch 1 |  iter 4301 / 9295 | time 136[s] | loss 2.38\n",
      "| epoch 1 |  iter 4321 / 9295 | time 136[s] | loss 2.39\n",
      "| epoch 1 |  iter 4341 / 9295 | time 137[s] | loss 2.33\n",
      "| epoch 1 |  iter 4361 / 9295 | time 138[s] | loss 2.36\n",
      "| epoch 1 |  iter 4381 / 9295 | time 138[s] | loss 2.37\n",
      "| epoch 1 |  iter 4401 / 9295 | time 139[s] | loss 2.35\n",
      "| epoch 1 |  iter 4421 / 9295 | time 139[s] | loss 2.39\n",
      "| epoch 1 |  iter 4441 / 9295 | time 140[s] | loss 2.38\n",
      "| epoch 1 |  iter 4461 / 9295 | time 141[s] | loss 2.41\n",
      "| epoch 1 |  iter 4481 / 9295 | time 141[s] | loss 2.37\n",
      "| epoch 1 |  iter 4501 / 9295 | time 142[s] | loss 2.37\n",
      "| epoch 1 |  iter 4521 / 9295 | time 143[s] | loss 2.40\n",
      "| epoch 1 |  iter 4541 / 9295 | time 143[s] | loss 2.40\n",
      "| epoch 1 |  iter 4561 / 9295 | time 144[s] | loss 2.37\n",
      "| epoch 1 |  iter 4581 / 9295 | time 144[s] | loss 2.35\n",
      "| epoch 1 |  iter 4601 / 9295 | time 145[s] | loss 2.35\n",
      "| epoch 1 |  iter 4621 / 9295 | time 146[s] | loss 2.38\n",
      "| epoch 1 |  iter 4641 / 9295 | time 146[s] | loss 2.39\n",
      "| epoch 1 |  iter 4661 / 9295 | time 147[s] | loss 2.39\n",
      "| epoch 1 |  iter 4681 / 9295 | time 148[s] | loss 2.36\n",
      "| epoch 1 |  iter 4701 / 9295 | time 148[s] | loss 2.33\n",
      "| epoch 1 |  iter 4721 / 9295 | time 149[s] | loss 2.34\n",
      "| epoch 1 |  iter 4741 / 9295 | time 150[s] | loss 2.37\n",
      "| epoch 1 |  iter 4761 / 9295 | time 150[s] | loss 2.39\n",
      "| epoch 1 |  iter 4781 / 9295 | time 151[s] | loss 2.37\n",
      "| epoch 1 |  iter 4801 / 9295 | time 152[s] | loss 2.39\n",
      "| epoch 1 |  iter 4821 / 9295 | time 152[s] | loss 2.36\n",
      "| epoch 1 |  iter 4841 / 9295 | time 153[s] | loss 2.37\n",
      "| epoch 1 |  iter 4861 / 9295 | time 153[s] | loss 2.36\n",
      "| epoch 1 |  iter 4881 / 9295 | time 154[s] | loss 2.35\n",
      "| epoch 1 |  iter 4901 / 9295 | time 155[s] | loss 2.35\n",
      "| epoch 1 |  iter 4921 / 9295 | time 155[s] | loss 2.35\n",
      "| epoch 1 |  iter 4941 / 9295 | time 156[s] | loss 2.38\n",
      "| epoch 1 |  iter 4961 / 9295 | time 157[s] | loss 2.35\n",
      "| epoch 1 |  iter 4981 / 9295 | time 157[s] | loss 2.33\n",
      "| epoch 1 |  iter 5001 / 9295 | time 158[s] | loss 2.36\n",
      "| epoch 1 |  iter 5021 / 9295 | time 159[s] | loss 2.35\n",
      "| epoch 1 |  iter 5041 / 9295 | time 159[s] | loss 2.35\n",
      "| epoch 1 |  iter 5061 / 9295 | time 160[s] | loss 2.35\n",
      "| epoch 1 |  iter 5081 / 9295 | time 160[s] | loss 2.37\n",
      "| epoch 1 |  iter 5101 / 9295 | time 161[s] | loss 2.34\n",
      "| epoch 1 |  iter 5121 / 9295 | time 162[s] | loss 2.33\n",
      "| epoch 1 |  iter 5141 / 9295 | time 162[s] | loss 2.32\n",
      "| epoch 1 |  iter 5161 / 9295 | time 163[s] | loss 2.36\n",
      "| epoch 1 |  iter 5181 / 9295 | time 164[s] | loss 2.36\n",
      "| epoch 1 |  iter 5201 / 9295 | time 164[s] | loss 2.35\n",
      "| epoch 1 |  iter 5221 / 9295 | time 165[s] | loss 2.33\n",
      "| epoch 1 |  iter 5241 / 9295 | time 166[s] | loss 2.35\n",
      "| epoch 1 |  iter 5261 / 9295 | time 166[s] | loss 2.38\n",
      "| epoch 1 |  iter 5281 / 9295 | time 167[s] | loss 2.34\n",
      "| epoch 1 |  iter 5301 / 9295 | time 168[s] | loss 2.32\n",
      "| epoch 1 |  iter 5321 / 9295 | time 168[s] | loss 2.35\n",
      "| epoch 1 |  iter 5341 / 9295 | time 169[s] | loss 2.35\n",
      "| epoch 1 |  iter 5361 / 9295 | time 169[s] | loss 2.34\n",
      "| epoch 1 |  iter 5381 / 9295 | time 170[s] | loss 2.33\n",
      "| epoch 1 |  iter 5401 / 9295 | time 171[s] | loss 2.34\n",
      "| epoch 1 |  iter 5421 / 9295 | time 171[s] | loss 2.35\n",
      "| epoch 1 |  iter 5441 / 9295 | time 172[s] | loss 2.33\n",
      "| epoch 1 |  iter 5461 / 9295 | time 173[s] | loss 2.35\n",
      "| epoch 1 |  iter 5481 / 9295 | time 173[s] | loss 2.34\n",
      "| epoch 1 |  iter 5501 / 9295 | time 174[s] | loss 2.36\n",
      "| epoch 1 |  iter 5521 / 9295 | time 175[s] | loss 2.29\n",
      "| epoch 1 |  iter 5541 / 9295 | time 175[s] | loss 2.39\n",
      "| epoch 1 |  iter 5561 / 9295 | time 176[s] | loss 2.33\n",
      "| epoch 1 |  iter 5581 / 9295 | time 177[s] | loss 2.35\n",
      "| epoch 1 |  iter 5601 / 9295 | time 177[s] | loss 2.34\n",
      "| epoch 1 |  iter 5621 / 9295 | time 178[s] | loss 2.31\n",
      "| epoch 1 |  iter 5641 / 9295 | time 178[s] | loss 2.34\n",
      "| epoch 1 |  iter 5661 / 9295 | time 179[s] | loss 2.32\n",
      "| epoch 1 |  iter 5681 / 9295 | time 180[s] | loss 2.35\n",
      "| epoch 1 |  iter 5701 / 9295 | time 180[s] | loss 2.32\n",
      "| epoch 1 |  iter 5721 / 9295 | time 181[s] | loss 2.32\n",
      "| epoch 1 |  iter 5741 / 9295 | time 182[s] | loss 2.33\n",
      "| epoch 1 |  iter 5761 / 9295 | time 182[s] | loss 2.37\n",
      "| epoch 1 |  iter 5781 / 9295 | time 183[s] | loss 2.31\n",
      "| epoch 1 |  iter 5801 / 9295 | time 184[s] | loss 2.32\n",
      "| epoch 1 |  iter 5821 / 9295 | time 184[s] | loss 2.30\n",
      "| epoch 1 |  iter 5841 / 9295 | time 185[s] | loss 2.33\n",
      "| epoch 1 |  iter 5861 / 9295 | time 186[s] | loss 2.30\n",
      "| epoch 1 |  iter 5881 / 9295 | time 186[s] | loss 2.31\n",
      "| epoch 1 |  iter 5901 / 9295 | time 187[s] | loss 2.32\n",
      "| epoch 1 |  iter 5921 / 9295 | time 187[s] | loss 2.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 5941 / 9295 | time 188[s] | loss 2.30\n",
      "| epoch 1 |  iter 5961 / 9295 | time 189[s] | loss 2.31\n",
      "| epoch 1 |  iter 5981 / 9295 | time 189[s] | loss 2.36\n",
      "| epoch 1 |  iter 6001 / 9295 | time 190[s] | loss 2.35\n",
      "| epoch 1 |  iter 6021 / 9295 | time 191[s] | loss 2.33\n",
      "| epoch 1 |  iter 6041 / 9295 | time 191[s] | loss 2.32\n",
      "| epoch 1 |  iter 6061 / 9295 | time 192[s] | loss 2.29\n",
      "| epoch 1 |  iter 6081 / 9295 | time 193[s] | loss 2.33\n",
      "| epoch 1 |  iter 6101 / 9295 | time 193[s] | loss 2.34\n",
      "| epoch 1 |  iter 6121 / 9295 | time 194[s] | loss 2.34\n",
      "| epoch 1 |  iter 6141 / 9295 | time 194[s] | loss 2.30\n",
      "| epoch 1 |  iter 6161 / 9295 | time 195[s] | loss 2.30\n",
      "| epoch 1 |  iter 6181 / 9295 | time 196[s] | loss 2.31\n",
      "| epoch 1 |  iter 6201 / 9295 | time 196[s] | loss 2.29\n",
      "| epoch 1 |  iter 6221 / 9295 | time 197[s] | loss 2.32\n",
      "| epoch 1 |  iter 6241 / 9295 | time 198[s] | loss 2.31\n",
      "| epoch 1 |  iter 6261 / 9295 | time 198[s] | loss 2.36\n",
      "| epoch 1 |  iter 6281 / 9295 | time 199[s] | loss 2.29\n",
      "| epoch 1 |  iter 6301 / 9295 | time 200[s] | loss 2.34\n",
      "| epoch 1 |  iter 6321 / 9295 | time 200[s] | loss 2.30\n",
      "| epoch 1 |  iter 6341 / 9295 | time 201[s] | loss 2.32\n",
      "| epoch 1 |  iter 6361 / 9295 | time 202[s] | loss 2.30\n",
      "| epoch 1 |  iter 6381 / 9295 | time 202[s] | loss 2.30\n",
      "| epoch 1 |  iter 6401 / 9295 | time 203[s] | loss 2.31\n",
      "| epoch 1 |  iter 6421 / 9295 | time 203[s] | loss 2.28\n",
      "| epoch 1 |  iter 6441 / 9295 | time 204[s] | loss 2.26\n",
      "| epoch 1 |  iter 6461 / 9295 | time 205[s] | loss 2.31\n",
      "| epoch 1 |  iter 6481 / 9295 | time 205[s] | loss 2.35\n",
      "| epoch 1 |  iter 6501 / 9295 | time 206[s] | loss 2.32\n",
      "| epoch 1 |  iter 6521 / 9295 | time 207[s] | loss 2.30\n",
      "| epoch 1 |  iter 6541 / 9295 | time 208[s] | loss 2.31\n",
      "| epoch 1 |  iter 6561 / 9295 | time 208[s] | loss 2.27\n",
      "| epoch 1 |  iter 6581 / 9295 | time 209[s] | loss 2.30\n",
      "| epoch 1 |  iter 6601 / 9295 | time 210[s] | loss 2.30\n",
      "| epoch 1 |  iter 6621 / 9295 | time 210[s] | loss 2.30\n",
      "| epoch 1 |  iter 6641 / 9295 | time 211[s] | loss 2.29\n",
      "| epoch 1 |  iter 6661 / 9295 | time 212[s] | loss 2.34\n",
      "| epoch 1 |  iter 6681 / 9295 | time 212[s] | loss 2.32\n",
      "| epoch 1 |  iter 6701 / 9295 | time 213[s] | loss 2.27\n",
      "| epoch 1 |  iter 6721 / 9295 | time 213[s] | loss 2.30\n",
      "| epoch 1 |  iter 6741 / 9295 | time 214[s] | loss 2.29\n",
      "| epoch 1 |  iter 6761 / 9295 | time 215[s] | loss 2.33\n",
      "| epoch 1 |  iter 6781 / 9295 | time 215[s] | loss 2.30\n",
      "| epoch 1 |  iter 6801 / 9295 | time 216[s] | loss 2.29\n",
      "| epoch 1 |  iter 6821 / 9295 | time 217[s] | loss 2.28\n",
      "| epoch 1 |  iter 6841 / 9295 | time 217[s] | loss 2.32\n",
      "| epoch 1 |  iter 6861 / 9295 | time 218[s] | loss 2.32\n",
      "| epoch 1 |  iter 6881 / 9295 | time 219[s] | loss 2.27\n",
      "| epoch 1 |  iter 6901 / 9295 | time 219[s] | loss 2.28\n",
      "| epoch 1 |  iter 6921 / 9295 | time 220[s] | loss 2.30\n",
      "| epoch 1 |  iter 6941 / 9295 | time 220[s] | loss 2.29\n",
      "| epoch 1 |  iter 6961 / 9295 | time 221[s] | loss 2.28\n",
      "| epoch 1 |  iter 6981 / 9295 | time 222[s] | loss 2.28\n",
      "| epoch 1 |  iter 7001 / 9295 | time 222[s] | loss 2.29\n",
      "| epoch 1 |  iter 7021 / 9295 | time 223[s] | loss 2.31\n",
      "| epoch 1 |  iter 7041 / 9295 | time 224[s] | loss 2.29\n",
      "| epoch 1 |  iter 7061 / 9295 | time 224[s] | loss 2.31\n",
      "| epoch 1 |  iter 7081 / 9295 | time 225[s] | loss 2.28\n",
      "| epoch 1 |  iter 7101 / 9295 | time 226[s] | loss 2.29\n",
      "| epoch 1 |  iter 7121 / 9295 | time 226[s] | loss 2.29\n",
      "| epoch 1 |  iter 7141 / 9295 | time 227[s] | loss 2.28\n",
      "| epoch 1 |  iter 7161 / 9295 | time 227[s] | loss 2.29\n",
      "| epoch 1 |  iter 7181 / 9295 | time 228[s] | loss 2.31\n",
      "| epoch 1 |  iter 7201 / 9295 | time 229[s] | loss 2.28\n",
      "| epoch 1 |  iter 7221 / 9295 | time 229[s] | loss 2.28\n",
      "| epoch 1 |  iter 7241 / 9295 | time 230[s] | loss 2.27\n",
      "| epoch 1 |  iter 7261 / 9295 | time 231[s] | loss 2.29\n",
      "| epoch 1 |  iter 7281 / 9295 | time 231[s] | loss 2.27\n",
      "| epoch 1 |  iter 7301 / 9295 | time 232[s] | loss 2.26\n",
      "| epoch 1 |  iter 7321 / 9295 | time 233[s] | loss 2.27\n",
      "| epoch 1 |  iter 7341 / 9295 | time 233[s] | loss 2.27\n",
      "| epoch 1 |  iter 7361 / 9295 | time 234[s] | loss 2.25\n",
      "| epoch 1 |  iter 7381 / 9295 | time 235[s] | loss 2.28\n",
      "| epoch 1 |  iter 7401 / 9295 | time 235[s] | loss 2.25\n",
      "| epoch 1 |  iter 7421 / 9295 | time 236[s] | loss 2.25\n",
      "| epoch 1 |  iter 7441 / 9295 | time 236[s] | loss 2.30\n",
      "| epoch 1 |  iter 7461 / 9295 | time 237[s] | loss 2.30\n",
      "| epoch 1 |  iter 7481 / 9295 | time 238[s] | loss 2.26\n",
      "| epoch 1 |  iter 7501 / 9295 | time 238[s] | loss 2.24\n",
      "| epoch 1 |  iter 7521 / 9295 | time 239[s] | loss 2.26\n",
      "| epoch 1 |  iter 7541 / 9295 | time 240[s] | loss 2.28\n",
      "| epoch 1 |  iter 7561 / 9295 | time 240[s] | loss 2.23\n",
      "| epoch 1 |  iter 7581 / 9295 | time 241[s] | loss 2.27\n",
      "| epoch 1 |  iter 7601 / 9295 | time 242[s] | loss 2.26\n",
      "| epoch 1 |  iter 7621 / 9295 | time 242[s] | loss 2.26\n",
      "| epoch 1 |  iter 7641 / 9295 | time 243[s] | loss 2.26\n",
      "| epoch 1 |  iter 7661 / 9295 | time 243[s] | loss 2.25\n",
      "| epoch 1 |  iter 7681 / 9295 | time 244[s] | loss 2.24\n",
      "| epoch 1 |  iter 7701 / 9295 | time 245[s] | loss 2.26\n",
      "| epoch 1 |  iter 7721 / 9295 | time 245[s] | loss 2.27\n",
      "| epoch 1 |  iter 7741 / 9295 | time 246[s] | loss 2.24\n",
      "| epoch 1 |  iter 7761 / 9295 | time 247[s] | loss 2.24\n",
      "| epoch 1 |  iter 7781 / 9295 | time 247[s] | loss 2.26\n",
      "| epoch 1 |  iter 7801 / 9295 | time 248[s] | loss 2.24\n",
      "| epoch 1 |  iter 7821 / 9295 | time 249[s] | loss 2.29\n",
      "| epoch 1 |  iter 7841 / 9295 | time 249[s] | loss 2.27\n",
      "| epoch 1 |  iter 7861 / 9295 | time 250[s] | loss 2.23\n",
      "| epoch 1 |  iter 7881 / 9295 | time 251[s] | loss 2.29\n",
      "| epoch 1 |  iter 7901 / 9295 | time 251[s] | loss 2.27\n",
      "| epoch 1 |  iter 7921 / 9295 | time 252[s] | loss 2.27\n",
      "| epoch 1 |  iter 7941 / 9295 | time 253[s] | loss 2.23\n",
      "| epoch 1 |  iter 7961 / 9295 | time 253[s] | loss 2.24\n",
      "| epoch 1 |  iter 7981 / 9295 | time 254[s] | loss 2.29\n",
      "| epoch 1 |  iter 8001 / 9295 | time 254[s] | loss 2.26\n",
      "| epoch 1 |  iter 8021 / 9295 | time 255[s] | loss 2.26\n",
      "| epoch 1 |  iter 8041 / 9295 | time 256[s] | loss 2.26\n",
      "| epoch 1 |  iter 8061 / 9295 | time 256[s] | loss 2.25\n",
      "| epoch 1 |  iter 8081 / 9295 | time 257[s] | loss 2.27\n",
      "| epoch 1 |  iter 8101 / 9295 | time 258[s] | loss 2.27\n",
      "| epoch 1 |  iter 8121 / 9295 | time 258[s] | loss 2.25\n",
      "| epoch 1 |  iter 8141 / 9295 | time 259[s] | loss 2.26\n",
      "| epoch 1 |  iter 8161 / 9295 | time 260[s] | loss 2.26\n",
      "| epoch 1 |  iter 8181 / 9295 | time 260[s] | loss 2.24\n",
      "| epoch 1 |  iter 8201 / 9295 | time 261[s] | loss 2.27\n",
      "| epoch 1 |  iter 8221 / 9295 | time 262[s] | loss 2.25\n",
      "| epoch 1 |  iter 8241 / 9295 | time 262[s] | loss 2.23\n",
      "| epoch 1 |  iter 8261 / 9295 | time 263[s] | loss 2.21\n",
      "| epoch 1 |  iter 8281 / 9295 | time 263[s] | loss 2.26\n",
      "| epoch 1 |  iter 8301 / 9295 | time 264[s] | loss 2.24\n",
      "| epoch 1 |  iter 8321 / 9295 | time 265[s] | loss 2.22\n",
      "| epoch 1 |  iter 8341 / 9295 | time 265[s] | loss 2.25\n",
      "| epoch 1 |  iter 8361 / 9295 | time 266[s] | loss 2.24\n",
      "| epoch 1 |  iter 8381 / 9295 | time 267[s] | loss 2.27\n",
      "| epoch 1 |  iter 8401 / 9295 | time 267[s] | loss 2.29\n",
      "| epoch 1 |  iter 8421 / 9295 | time 268[s] | loss 2.23\n",
      "| epoch 1 |  iter 8441 / 9295 | time 269[s] | loss 2.26\n",
      "| epoch 1 |  iter 8461 / 9295 | time 269[s] | loss 2.24\n",
      "| epoch 1 |  iter 8481 / 9295 | time 270[s] | loss 2.24\n",
      "| epoch 1 |  iter 8501 / 9295 | time 270[s] | loss 2.21\n",
      "| epoch 1 |  iter 8521 / 9295 | time 271[s] | loss 2.18\n",
      "| epoch 1 |  iter 8541 / 9295 | time 272[s] | loss 2.24\n",
      "| epoch 1 |  iter 8561 / 9295 | time 272[s] | loss 2.25\n",
      "| epoch 1 |  iter 8581 / 9295 | time 273[s] | loss 2.24\n",
      "| epoch 1 |  iter 8601 / 9295 | time 274[s] | loss 2.25\n",
      "| epoch 1 |  iter 8621 / 9295 | time 274[s] | loss 2.25\n",
      "| epoch 1 |  iter 8641 / 9295 | time 275[s] | loss 2.22\n",
      "| epoch 1 |  iter 8661 / 9295 | time 276[s] | loss 2.24\n",
      "| epoch 1 |  iter 8681 / 9295 | time 276[s] | loss 2.23\n",
      "| epoch 1 |  iter 8701 / 9295 | time 277[s] | loss 2.24\n",
      "| epoch 1 |  iter 8721 / 9295 | time 278[s] | loss 2.22\n",
      "| epoch 1 |  iter 8741 / 9295 | time 278[s] | loss 2.23\n",
      "| epoch 1 |  iter 8761 / 9295 | time 279[s] | loss 2.23\n",
      "| epoch 1 |  iter 8781 / 9295 | time 279[s] | loss 2.23\n",
      "| epoch 1 |  iter 8801 / 9295 | time 280[s] | loss 2.22\n",
      "| epoch 1 |  iter 8821 / 9295 | time 281[s] | loss 2.25\n",
      "| epoch 1 |  iter 8841 / 9295 | time 281[s] | loss 2.25\n",
      "| epoch 1 |  iter 8861 / 9295 | time 282[s] | loss 2.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 8881 / 9295 | time 283[s] | loss 2.24\n",
      "| epoch 1 |  iter 8901 / 9295 | time 283[s] | loss 2.21\n",
      "| epoch 1 |  iter 8921 / 9295 | time 284[s] | loss 2.24\n",
      "| epoch 1 |  iter 8941 / 9295 | time 285[s] | loss 2.24\n",
      "| epoch 1 |  iter 8961 / 9295 | time 285[s] | loss 2.20\n",
      "| epoch 1 |  iter 8981 / 9295 | time 286[s] | loss 2.22\n",
      "| epoch 1 |  iter 9001 / 9295 | time 287[s] | loss 2.21\n",
      "| epoch 1 |  iter 9021 / 9295 | time 287[s] | loss 2.21\n",
      "| epoch 1 |  iter 9041 / 9295 | time 288[s] | loss 2.24\n",
      "| epoch 1 |  iter 9061 / 9295 | time 288[s] | loss 2.25\n",
      "| epoch 1 |  iter 9081 / 9295 | time 289[s] | loss 2.27\n",
      "| epoch 1 |  iter 9101 / 9295 | time 290[s] | loss 2.21\n",
      "| epoch 1 |  iter 9121 / 9295 | time 290[s] | loss 2.26\n",
      "| epoch 1 |  iter 9141 / 9295 | time 291[s] | loss 2.22\n",
      "| epoch 1 |  iter 9161 / 9295 | time 292[s] | loss 2.23\n",
      "| epoch 1 |  iter 9181 / 9295 | time 292[s] | loss 2.20\n",
      "| epoch 1 |  iter 9201 / 9295 | time 293[s] | loss 2.23\n",
      "| epoch 1 |  iter 9221 / 9295 | time 294[s] | loss 2.23\n",
      "| epoch 1 |  iter 9241 / 9295 | time 294[s] | loss 2.20\n",
      "| epoch 1 |  iter 9261 / 9295 | time 295[s] | loss 2.22\n",
      "| epoch 1 |  iter 9281 / 9295 | time 296[s] | loss 2.21\n",
      "| epoch 2 |  iter 1 / 9295 | time 296[s] | loss 2.21\n",
      "| epoch 2 |  iter 21 / 9295 | time 297[s] | loss 2.18\n",
      "| epoch 2 |  iter 41 / 9295 | time 297[s] | loss 2.14\n",
      "| epoch 2 |  iter 61 / 9295 | time 298[s] | loss 2.18\n",
      "| epoch 2 |  iter 81 / 9295 | time 299[s] | loss 2.19\n",
      "| epoch 2 |  iter 101 / 9295 | time 299[s] | loss 2.18\n",
      "| epoch 2 |  iter 121 / 9295 | time 300[s] | loss 2.18\n",
      "| epoch 2 |  iter 141 / 9295 | time 301[s] | loss 2.16\n",
      "| epoch 2 |  iter 161 / 9295 | time 301[s] | loss 2.18\n",
      "| epoch 2 |  iter 181 / 9295 | time 302[s] | loss 2.17\n",
      "| epoch 2 |  iter 201 / 9295 | time 303[s] | loss 2.19\n",
      "| epoch 2 |  iter 221 / 9295 | time 303[s] | loss 2.20\n",
      "| epoch 2 |  iter 241 / 9295 | time 304[s] | loss 2.15\n",
      "| epoch 2 |  iter 261 / 9295 | time 304[s] | loss 2.18\n",
      "| epoch 2 |  iter 281 / 9295 | time 305[s] | loss 2.19\n",
      "| epoch 2 |  iter 301 / 9295 | time 306[s] | loss 2.18\n",
      "| epoch 2 |  iter 321 / 9295 | time 306[s] | loss 2.19\n",
      "| epoch 2 |  iter 341 / 9295 | time 307[s] | loss 2.18\n",
      "| epoch 2 |  iter 361 / 9295 | time 308[s] | loss 2.18\n",
      "| epoch 2 |  iter 381 / 9295 | time 308[s] | loss 2.17\n",
      "| epoch 2 |  iter 401 / 9295 | time 309[s] | loss 2.17\n",
      "| epoch 2 |  iter 421 / 9295 | time 310[s] | loss 2.16\n",
      "| epoch 2 |  iter 441 / 9295 | time 310[s] | loss 2.15\n",
      "| epoch 2 |  iter 461 / 9295 | time 311[s] | loss 2.17\n",
      "| epoch 2 |  iter 481 / 9295 | time 311[s] | loss 2.19\n",
      "| epoch 2 |  iter 501 / 9295 | time 312[s] | loss 2.19\n",
      "| epoch 2 |  iter 521 / 9295 | time 313[s] | loss 2.13\n",
      "| epoch 2 |  iter 541 / 9295 | time 313[s] | loss 2.19\n",
      "| epoch 2 |  iter 561 / 9295 | time 314[s] | loss 2.19\n",
      "| epoch 2 |  iter 581 / 9295 | time 315[s] | loss 2.16\n",
      "| epoch 2 |  iter 601 / 9295 | time 315[s] | loss 2.16\n",
      "| epoch 2 |  iter 621 / 9295 | time 316[s] | loss 2.17\n",
      "| epoch 2 |  iter 641 / 9295 | time 317[s] | loss 2.16\n",
      "| epoch 2 |  iter 661 / 9295 | time 317[s] | loss 2.17\n",
      "| epoch 2 |  iter 681 / 9295 | time 318[s] | loss 2.17\n",
      "| epoch 2 |  iter 701 / 9295 | time 319[s] | loss 2.17\n",
      "| epoch 2 |  iter 721 / 9295 | time 319[s] | loss 2.17\n",
      "| epoch 2 |  iter 741 / 9295 | time 320[s] | loss 2.17\n",
      "| epoch 2 |  iter 761 / 9295 | time 320[s] | loss 2.16\n",
      "| epoch 2 |  iter 781 / 9295 | time 321[s] | loss 2.18\n",
      "| epoch 2 |  iter 801 / 9295 | time 322[s] | loss 2.14\n",
      "| epoch 2 |  iter 821 / 9295 | time 322[s] | loss 2.19\n",
      "| epoch 2 |  iter 841 / 9295 | time 323[s] | loss 2.16\n",
      "| epoch 2 |  iter 861 / 9295 | time 324[s] | loss 2.16\n",
      "| epoch 2 |  iter 881 / 9295 | time 324[s] | loss 2.16\n",
      "| epoch 2 |  iter 901 / 9295 | time 325[s] | loss 2.16\n",
      "| epoch 2 |  iter 921 / 9295 | time 325[s] | loss 2.20\n",
      "| epoch 2 |  iter 941 / 9295 | time 326[s] | loss 2.16\n",
      "| epoch 2 |  iter 961 / 9295 | time 327[s] | loss 2.19\n",
      "| epoch 2 |  iter 981 / 9295 | time 327[s] | loss 2.17\n",
      "| epoch 2 |  iter 1001 / 9295 | time 328[s] | loss 2.16\n",
      "| epoch 2 |  iter 1021 / 9295 | time 329[s] | loss 2.15\n",
      "| epoch 2 |  iter 1041 / 9295 | time 329[s] | loss 2.20\n",
      "| epoch 2 |  iter 1061 / 9295 | time 330[s] | loss 2.15\n",
      "| epoch 2 |  iter 1081 / 9295 | time 331[s] | loss 2.17\n",
      "| epoch 2 |  iter 1101 / 9295 | time 331[s] | loss 2.16\n",
      "| epoch 2 |  iter 1121 / 9295 | time 332[s] | loss 2.15\n",
      "| epoch 2 |  iter 1141 / 9295 | time 332[s] | loss 2.16\n",
      "| epoch 2 |  iter 1161 / 9295 | time 333[s] | loss 2.13\n",
      "| epoch 2 |  iter 1181 / 9295 | time 334[s] | loss 2.16\n",
      "| epoch 2 |  iter 1201 / 9295 | time 334[s] | loss 2.13\n",
      "| epoch 2 |  iter 1221 / 9295 | time 335[s] | loss 2.11\n",
      "| epoch 2 |  iter 1241 / 9295 | time 336[s] | loss 2.16\n",
      "| epoch 2 |  iter 1261 / 9295 | time 336[s] | loss 2.18\n",
      "| epoch 2 |  iter 1281 / 9295 | time 337[s] | loss 2.15\n",
      "| epoch 2 |  iter 1301 / 9295 | time 338[s] | loss 2.15\n",
      "| epoch 2 |  iter 1321 / 9295 | time 338[s] | loss 2.16\n",
      "| epoch 2 |  iter 1341 / 9295 | time 339[s] | loss 2.18\n",
      "| epoch 2 |  iter 1361 / 9295 | time 340[s] | loss 2.15\n",
      "| epoch 2 |  iter 1381 / 9295 | time 340[s] | loss 2.14\n",
      "| epoch 2 |  iter 1401 / 9295 | time 341[s] | loss 2.13\n",
      "| epoch 2 |  iter 1421 / 9295 | time 341[s] | loss 2.14\n",
      "| epoch 2 |  iter 1441 / 9295 | time 342[s] | loss 2.14\n",
      "| epoch 2 |  iter 1461 / 9295 | time 343[s] | loss 2.13\n",
      "| epoch 2 |  iter 1481 / 9295 | time 343[s] | loss 2.16\n",
      "| epoch 2 |  iter 1501 / 9295 | time 344[s] | loss 2.14\n",
      "| epoch 2 |  iter 1521 / 9295 | time 345[s] | loss 2.16\n",
      "| epoch 2 |  iter 1541 / 9295 | time 345[s] | loss 2.14\n",
      "| epoch 2 |  iter 1561 / 9295 | time 346[s] | loss 2.12\n",
      "| epoch 2 |  iter 1581 / 9295 | time 347[s] | loss 2.17\n",
      "| epoch 2 |  iter 1601 / 9295 | time 347[s] | loss 2.17\n",
      "| epoch 2 |  iter 1621 / 9295 | time 348[s] | loss 2.17\n",
      "| epoch 2 |  iter 1641 / 9295 | time 348[s] | loss 2.17\n",
      "| epoch 2 |  iter 1661 / 9295 | time 349[s] | loss 2.12\n",
      "| epoch 2 |  iter 1681 / 9295 | time 350[s] | loss 2.14\n",
      "| epoch 2 |  iter 1701 / 9295 | time 350[s] | loss 2.17\n",
      "| epoch 2 |  iter 1721 / 9295 | time 351[s] | loss 2.12\n",
      "| epoch 2 |  iter 1741 / 9295 | time 352[s] | loss 2.17\n",
      "| epoch 2 |  iter 1761 / 9295 | time 352[s] | loss 2.18\n",
      "| epoch 2 |  iter 1781 / 9295 | time 353[s] | loss 2.12\n",
      "| epoch 2 |  iter 1801 / 9295 | time 354[s] | loss 2.13\n",
      "| epoch 2 |  iter 1821 / 9295 | time 354[s] | loss 2.12\n",
      "| epoch 2 |  iter 1841 / 9295 | time 355[s] | loss 2.12\n",
      "| epoch 2 |  iter 1861 / 9295 | time 356[s] | loss 2.16\n",
      "| epoch 2 |  iter 1881 / 9295 | time 356[s] | loss 2.16\n",
      "| epoch 2 |  iter 1901 / 9295 | time 357[s] | loss 2.15\n",
      "| epoch 2 |  iter 1921 / 9295 | time 357[s] | loss 2.12\n",
      "| epoch 2 |  iter 1941 / 9295 | time 358[s] | loss 2.17\n",
      "| epoch 2 |  iter 1961 / 9295 | time 359[s] | loss 2.10\n",
      "| epoch 2 |  iter 1981 / 9295 | time 359[s] | loss 2.13\n",
      "| epoch 2 |  iter 2001 / 9295 | time 360[s] | loss 2.14\n",
      "| epoch 2 |  iter 2021 / 9295 | time 361[s] | loss 2.12\n",
      "| epoch 2 |  iter 2041 / 9295 | time 361[s] | loss 2.09\n",
      "| epoch 2 |  iter 2061 / 9295 | time 362[s] | loss 2.14\n",
      "| epoch 2 |  iter 2081 / 9295 | time 362[s] | loss 2.14\n",
      "| epoch 2 |  iter 2101 / 9295 | time 363[s] | loss 2.12\n",
      "| epoch 2 |  iter 2121 / 9295 | time 364[s] | loss 2.14\n",
      "| epoch 2 |  iter 2141 / 9295 | time 364[s] | loss 2.14\n",
      "| epoch 2 |  iter 2161 / 9295 | time 365[s] | loss 2.16\n",
      "| epoch 2 |  iter 2181 / 9295 | time 366[s] | loss 2.15\n",
      "| epoch 2 |  iter 2201 / 9295 | time 366[s] | loss 2.14\n",
      "| epoch 2 |  iter 2221 / 9295 | time 367[s] | loss 2.14\n",
      "| epoch 2 |  iter 2241 / 9295 | time 368[s] | loss 2.11\n",
      "| epoch 2 |  iter 2261 / 9295 | time 368[s] | loss 2.11\n",
      "| epoch 2 |  iter 2281 / 9295 | time 369[s] | loss 2.11\n",
      "| epoch 2 |  iter 2301 / 9295 | time 370[s] | loss 2.09\n",
      "| epoch 2 |  iter 2321 / 9295 | time 370[s] | loss 2.15\n",
      "| epoch 2 |  iter 2341 / 9295 | time 371[s] | loss 2.14\n",
      "| epoch 2 |  iter 2361 / 9295 | time 371[s] | loss 2.09\n",
      "| epoch 2 |  iter 2381 / 9295 | time 372[s] | loss 2.08\n",
      "| epoch 2 |  iter 2401 / 9295 | time 373[s] | loss 2.16\n",
      "| epoch 2 |  iter 2421 / 9295 | time 373[s] | loss 2.16\n",
      "| epoch 2 |  iter 2441 / 9295 | time 374[s] | loss 2.14\n",
      "| epoch 2 |  iter 2461 / 9295 | time 375[s] | loss 2.11\n",
      "| epoch 2 |  iter 2481 / 9295 | time 375[s] | loss 2.11\n",
      "| epoch 2 |  iter 2501 / 9295 | time 376[s] | loss 2.13\n",
      "| epoch 2 |  iter 2521 / 9295 | time 377[s] | loss 2.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 2 |  iter 2541 / 9295 | time 377[s] | loss 2.11\n",
      "| epoch 2 |  iter 2561 / 9295 | time 378[s] | loss 2.14\n",
      "| epoch 2 |  iter 2581 / 9295 | time 378[s] | loss 2.17\n",
      "| epoch 2 |  iter 2601 / 9295 | time 379[s] | loss 2.15\n",
      "| epoch 2 |  iter 2621 / 9295 | time 380[s] | loss 2.15\n",
      "| epoch 2 |  iter 2641 / 9295 | time 380[s] | loss 2.10\n",
      "| epoch 2 |  iter 2661 / 9295 | time 381[s] | loss 2.14\n",
      "| epoch 2 |  iter 2681 / 9295 | time 382[s] | loss 2.13\n",
      "| epoch 2 |  iter 2701 / 9295 | time 382[s] | loss 2.13\n",
      "| epoch 2 |  iter 2721 / 9295 | time 383[s] | loss 2.13\n",
      "| epoch 2 |  iter 2741 / 9295 | time 384[s] | loss 2.07\n",
      "| epoch 2 |  iter 2761 / 9295 | time 384[s] | loss 2.16\n",
      "| epoch 2 |  iter 2781 / 9295 | time 385[s] | loss 2.12\n",
      "| epoch 2 |  iter 2801 / 9295 | time 386[s] | loss 2.13\n",
      "| epoch 2 |  iter 2821 / 9295 | time 386[s] | loss 2.13\n",
      "| epoch 2 |  iter 2841 / 9295 | time 387[s] | loss 2.13\n",
      "| epoch 2 |  iter 2861 / 9295 | time 387[s] | loss 2.11\n",
      "| epoch 2 |  iter 2881 / 9295 | time 388[s] | loss 2.11\n",
      "| epoch 2 |  iter 2901 / 9295 | time 389[s] | loss 2.12\n",
      "| epoch 2 |  iter 2921 / 9295 | time 389[s] | loss 2.13\n",
      "| epoch 2 |  iter 2941 / 9295 | time 390[s] | loss 2.13\n",
      "| epoch 2 |  iter 2961 / 9295 | time 391[s] | loss 2.11\n",
      "| epoch 2 |  iter 2981 / 9295 | time 391[s] | loss 2.17\n",
      "| epoch 2 |  iter 3001 / 9295 | time 392[s] | loss 2.12\n",
      "| epoch 2 |  iter 3021 / 9295 | time 393[s] | loss 2.09\n",
      "| epoch 2 |  iter 3041 / 9295 | time 393[s] | loss 2.07\n",
      "| epoch 2 |  iter 3061 / 9295 | time 394[s] | loss 2.12\n",
      "| epoch 2 |  iter 3081 / 9295 | time 394[s] | loss 2.14\n",
      "| epoch 2 |  iter 3101 / 9295 | time 395[s] | loss 2.13\n",
      "| epoch 2 |  iter 3121 / 9295 | time 396[s] | loss 2.07\n",
      "| epoch 2 |  iter 3141 / 9295 | time 396[s] | loss 2.09\n",
      "| epoch 2 |  iter 3161 / 9295 | time 397[s] | loss 2.16\n",
      "| epoch 2 |  iter 3181 / 9295 | time 398[s] | loss 2.15\n",
      "| epoch 2 |  iter 3201 / 9295 | time 398[s] | loss 2.12\n",
      "| epoch 2 |  iter 3221 / 9295 | time 399[s] | loss 2.10\n",
      "| epoch 2 |  iter 3241 / 9295 | time 400[s] | loss 2.11\n",
      "| epoch 2 |  iter 3261 / 9295 | time 400[s] | loss 2.11\n",
      "| epoch 2 |  iter 3281 / 9295 | time 401[s] | loss 2.11\n",
      "| epoch 2 |  iter 3301 / 9295 | time 401[s] | loss 2.10\n",
      "| epoch 2 |  iter 3321 / 9295 | time 402[s] | loss 2.11\n",
      "| epoch 2 |  iter 3341 / 9295 | time 403[s] | loss 2.15\n",
      "| epoch 2 |  iter 3361 / 9295 | time 403[s] | loss 2.09\n",
      "| epoch 2 |  iter 3381 / 9295 | time 404[s] | loss 2.10\n",
      "| epoch 2 |  iter 3401 / 9295 | time 405[s] | loss 2.12\n",
      "| epoch 2 |  iter 3421 / 9295 | time 405[s] | loss 2.05\n",
      "| epoch 2 |  iter 3441 / 9295 | time 406[s] | loss 2.09\n",
      "| epoch 2 |  iter 3461 / 9295 | time 407[s] | loss 2.12\n",
      "| epoch 2 |  iter 3481 / 9295 | time 407[s] | loss 2.14\n",
      "| epoch 2 |  iter 3501 / 9295 | time 408[s] | loss 2.11\n",
      "| epoch 2 |  iter 3521 / 9295 | time 409[s] | loss 2.12\n",
      "| epoch 2 |  iter 3541 / 9295 | time 409[s] | loss 2.09\n",
      "| epoch 2 |  iter 3561 / 9295 | time 410[s] | loss 2.07\n",
      "| epoch 2 |  iter 3581 / 9295 | time 410[s] | loss 2.11\n",
      "| epoch 2 |  iter 3601 / 9295 | time 411[s] | loss 2.07\n",
      "| epoch 2 |  iter 3621 / 9295 | time 412[s] | loss 2.10\n",
      "| epoch 2 |  iter 3641 / 9295 | time 412[s] | loss 2.10\n",
      "| epoch 2 |  iter 3661 / 9295 | time 413[s] | loss 2.07\n",
      "| epoch 2 |  iter 3681 / 9295 | time 414[s] | loss 2.14\n",
      "| epoch 2 |  iter 3701 / 9295 | time 414[s] | loss 2.05\n",
      "| epoch 2 |  iter 3721 / 9295 | time 415[s] | loss 2.07\n",
      "| epoch 2 |  iter 3741 / 9295 | time 416[s] | loss 2.11\n",
      "| epoch 2 |  iter 3761 / 9295 | time 416[s] | loss 2.12\n",
      "| epoch 2 |  iter 3781 / 9295 | time 417[s] | loss 2.10\n",
      "| epoch 2 |  iter 3801 / 9295 | time 417[s] | loss 2.10\n",
      "| epoch 2 |  iter 3821 / 9295 | time 418[s] | loss 2.10\n",
      "| epoch 2 |  iter 3841 / 9295 | time 419[s] | loss 2.09\n",
      "| epoch 2 |  iter 3861 / 9295 | time 419[s] | loss 2.09\n",
      "| epoch 2 |  iter 3881 / 9295 | time 420[s] | loss 2.11\n",
      "| epoch 2 |  iter 3901 / 9295 | time 421[s] | loss 2.13\n",
      "| epoch 2 |  iter 3921 / 9295 | time 421[s] | loss 2.12\n",
      "| epoch 2 |  iter 3941 / 9295 | time 422[s] | loss 2.15\n",
      "| epoch 2 |  iter 3961 / 9295 | time 423[s] | loss 2.08\n",
      "| epoch 2 |  iter 3981 / 9295 | time 423[s] | loss 2.11\n",
      "| epoch 2 |  iter 4001 / 9295 | time 424[s] | loss 2.08\n",
      "| epoch 2 |  iter 4021 / 9295 | time 424[s] | loss 2.10\n",
      "| epoch 2 |  iter 4041 / 9295 | time 425[s] | loss 2.10\n",
      "| epoch 2 |  iter 4061 / 9295 | time 426[s] | loss 2.09\n",
      "| epoch 2 |  iter 4081 / 9295 | time 426[s] | loss 2.10\n",
      "| epoch 2 |  iter 4101 / 9295 | time 427[s] | loss 2.10\n",
      "| epoch 2 |  iter 4121 / 9295 | time 428[s] | loss 2.11\n",
      "| epoch 2 |  iter 4141 / 9295 | time 428[s] | loss 2.08\n",
      "| epoch 2 |  iter 4161 / 9295 | time 429[s] | loss 2.10\n",
      "| epoch 2 |  iter 4181 / 9295 | time 429[s] | loss 2.11\n",
      "| epoch 2 |  iter 4201 / 9295 | time 430[s] | loss 2.06\n",
      "| epoch 2 |  iter 4221 / 9295 | time 431[s] | loss 2.08\n",
      "| epoch 2 |  iter 4241 / 9295 | time 431[s] | loss 2.12\n",
      "| epoch 2 |  iter 4261 / 9295 | time 432[s] | loss 2.06\n",
      "| epoch 2 |  iter 4281 / 9295 | time 433[s] | loss 2.06\n",
      "| epoch 2 |  iter 4301 / 9295 | time 433[s] | loss 2.10\n",
      "| epoch 2 |  iter 4321 / 9295 | time 434[s] | loss 2.07\n",
      "| epoch 2 |  iter 4341 / 9295 | time 435[s] | loss 2.06\n",
      "| epoch 2 |  iter 4361 / 9295 | time 435[s] | loss 2.07\n",
      "| epoch 2 |  iter 4381 / 9295 | time 436[s] | loss 2.12\n",
      "| epoch 2 |  iter 4401 / 9295 | time 436[s] | loss 2.09\n",
      "| epoch 2 |  iter 4421 / 9295 | time 437[s] | loss 2.08\n",
      "| epoch 2 |  iter 4441 / 9295 | time 438[s] | loss 2.07\n",
      "| epoch 2 |  iter 4461 / 9295 | time 438[s] | loss 2.09\n",
      "| epoch 2 |  iter 4481 / 9295 | time 439[s] | loss 2.07\n",
      "| epoch 2 |  iter 4501 / 9295 | time 440[s] | loss 2.09\n",
      "| epoch 2 |  iter 4521 / 9295 | time 440[s] | loss 2.10\n",
      "| epoch 2 |  iter 4541 / 9295 | time 441[s] | loss 2.08\n",
      "| epoch 2 |  iter 4561 / 9295 | time 442[s] | loss 2.07\n",
      "| epoch 2 |  iter 4581 / 9295 | time 442[s] | loss 2.13\n",
      "| epoch 2 |  iter 4601 / 9295 | time 443[s] | loss 2.09\n",
      "| epoch 2 |  iter 4621 / 9295 | time 443[s] | loss 2.07\n",
      "| epoch 2 |  iter 4641 / 9295 | time 444[s] | loss 2.09\n",
      "| epoch 2 |  iter 4661 / 9295 | time 445[s] | loss 2.10\n",
      "| epoch 2 |  iter 4681 / 9295 | time 445[s] | loss 2.08\n",
      "| epoch 2 |  iter 4701 / 9295 | time 446[s] | loss 2.10\n",
      "| epoch 2 |  iter 4721 / 9295 | time 447[s] | loss 2.10\n",
      "| epoch 2 |  iter 4741 / 9295 | time 447[s] | loss 2.06\n",
      "| epoch 2 |  iter 4761 / 9295 | time 448[s] | loss 2.06\n",
      "| epoch 2 |  iter 4781 / 9295 | time 449[s] | loss 2.09\n",
      "| epoch 2 |  iter 4801 / 9295 | time 449[s] | loss 2.12\n",
      "| epoch 2 |  iter 4821 / 9295 | time 450[s] | loss 2.10\n",
      "| epoch 2 |  iter 4841 / 9295 | time 450[s] | loss 2.07\n",
      "| epoch 2 |  iter 4861 / 9295 | time 451[s] | loss 2.10\n",
      "| epoch 2 |  iter 4881 / 9295 | time 452[s] | loss 2.08\n",
      "| epoch 2 |  iter 4901 / 9295 | time 452[s] | loss 2.07\n",
      "| epoch 2 |  iter 4921 / 9295 | time 453[s] | loss 2.06\n",
      "| epoch 2 |  iter 4941 / 9295 | time 454[s] | loss 2.08\n",
      "| epoch 2 |  iter 4961 / 9295 | time 454[s] | loss 2.07\n",
      "| epoch 2 |  iter 4981 / 9295 | time 455[s] | loss 2.07\n",
      "| epoch 2 |  iter 5001 / 9295 | time 456[s] | loss 2.07\n",
      "| epoch 2 |  iter 5021 / 9295 | time 456[s] | loss 2.09\n",
      "| epoch 2 |  iter 5041 / 9295 | time 457[s] | loss 2.10\n",
      "| epoch 2 |  iter 5061 / 9295 | time 457[s] | loss 2.08\n",
      "| epoch 2 |  iter 5081 / 9295 | time 458[s] | loss 2.05\n",
      "| epoch 2 |  iter 5101 / 9295 | time 459[s] | loss 2.10\n",
      "| epoch 2 |  iter 5121 / 9295 | time 460[s] | loss 2.07\n",
      "| epoch 2 |  iter 5141 / 9295 | time 460[s] | loss 2.06\n",
      "| epoch 2 |  iter 5161 / 9295 | time 461[s] | loss 2.08\n",
      "| epoch 2 |  iter 5181 / 9295 | time 461[s] | loss 2.09\n",
      "| epoch 2 |  iter 5201 / 9295 | time 462[s] | loss 2.08\n",
      "| epoch 2 |  iter 5221 / 9295 | time 463[s] | loss 2.10\n",
      "| epoch 2 |  iter 5241 / 9295 | time 463[s] | loss 2.07\n",
      "| epoch 2 |  iter 5261 / 9295 | time 464[s] | loss 2.07\n",
      "| epoch 2 |  iter 5281 / 9295 | time 465[s] | loss 2.09\n",
      "| epoch 2 |  iter 5301 / 9295 | time 465[s] | loss 2.06\n",
      "| epoch 2 |  iter 5321 / 9295 | time 466[s] | loss 2.08\n",
      "| epoch 2 |  iter 5341 / 9295 | time 467[s] | loss 2.05\n",
      "| epoch 2 |  iter 5361 / 9295 | time 467[s] | loss 2.09\n",
      "| epoch 2 |  iter 5381 / 9295 | time 468[s] | loss 2.06\n",
      "| epoch 2 |  iter 5401 / 9295 | time 469[s] | loss 2.10\n",
      "| epoch 2 |  iter 5421 / 9295 | time 469[s] | loss 2.06\n",
      "| epoch 2 |  iter 5441 / 9295 | time 470[s] | loss 2.04\n",
      "| epoch 2 |  iter 5461 / 9295 | time 470[s] | loss 2.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 2 |  iter 5481 / 9295 | time 471[s] | loss 2.09\n",
      "| epoch 2 |  iter 5501 / 9295 | time 472[s] | loss 2.07\n",
      "| epoch 2 |  iter 5521 / 9295 | time 472[s] | loss 2.06\n",
      "| epoch 2 |  iter 5541 / 9295 | time 473[s] | loss 2.06\n",
      "| epoch 2 |  iter 5561 / 9295 | time 474[s] | loss 2.08\n",
      "| epoch 2 |  iter 5581 / 9295 | time 474[s] | loss 2.05\n",
      "| epoch 2 |  iter 5601 / 9295 | time 475[s] | loss 2.08\n",
      "| epoch 2 |  iter 5621 / 9295 | time 475[s] | loss 2.06\n",
      "| epoch 2 |  iter 5641 / 9295 | time 476[s] | loss 2.09\n",
      "| epoch 2 |  iter 5661 / 9295 | time 477[s] | loss 2.08\n",
      "| epoch 2 |  iter 5681 / 9295 | time 477[s] | loss 2.05\n",
      "| epoch 2 |  iter 5701 / 9295 | time 478[s] | loss 2.08\n",
      "| epoch 2 |  iter 5721 / 9295 | time 479[s] | loss 2.07\n",
      "| epoch 2 |  iter 5741 / 9295 | time 479[s] | loss 2.04\n",
      "| epoch 2 |  iter 5761 / 9295 | time 480[s] | loss 2.06\n",
      "| epoch 2 |  iter 5781 / 9295 | time 481[s] | loss 2.08\n",
      "| epoch 2 |  iter 5801 / 9295 | time 481[s] | loss 2.07\n",
      "| epoch 2 |  iter 5821 / 9295 | time 482[s] | loss 2.08\n",
      "| epoch 2 |  iter 5841 / 9295 | time 482[s] | loss 2.07\n",
      "| epoch 2 |  iter 5861 / 9295 | time 483[s] | loss 2.10\n",
      "| epoch 2 |  iter 5881 / 9295 | time 484[s] | loss 2.06\n",
      "| epoch 2 |  iter 5901 / 9295 | time 484[s] | loss 2.05\n",
      "| epoch 2 |  iter 5921 / 9295 | time 485[s] | loss 2.09\n",
      "| epoch 2 |  iter 5941 / 9295 | time 486[s] | loss 2.02\n",
      "| epoch 2 |  iter 5961 / 9295 | time 486[s] | loss 2.04\n",
      "| epoch 2 |  iter 5981 / 9295 | time 487[s] | loss 2.06\n",
      "| epoch 2 |  iter 6001 / 9295 | time 488[s] | loss 2.06\n",
      "| epoch 2 |  iter 6021 / 9295 | time 488[s] | loss 2.11\n",
      "| epoch 2 |  iter 6041 / 9295 | time 489[s] | loss 2.09\n",
      "| epoch 2 |  iter 6061 / 9295 | time 489[s] | loss 2.09\n",
      "| epoch 2 |  iter 6081 / 9295 | time 490[s] | loss 2.05\n",
      "| epoch 2 |  iter 6101 / 9295 | time 491[s] | loss 2.06\n",
      "| epoch 2 |  iter 6121 / 9295 | time 491[s] | loss 2.06\n",
      "| epoch 2 |  iter 6141 / 9295 | time 492[s] | loss 2.05\n",
      "| epoch 2 |  iter 6161 / 9295 | time 493[s] | loss 2.06\n",
      "| epoch 2 |  iter 6181 / 9295 | time 493[s] | loss 2.05\n",
      "| epoch 2 |  iter 6201 / 9295 | time 494[s] | loss 2.06\n",
      "| epoch 2 |  iter 6221 / 9295 | time 495[s] | loss 2.09\n",
      "| epoch 2 |  iter 6241 / 9295 | time 495[s] | loss 2.05\n",
      "| epoch 2 |  iter 6261 / 9295 | time 496[s] | loss 2.03\n",
      "| epoch 2 |  iter 6281 / 9295 | time 496[s] | loss 2.01\n",
      "| epoch 2 |  iter 6301 / 9295 | time 497[s] | loss 2.05\n",
      "| epoch 2 |  iter 6321 / 9295 | time 498[s] | loss 2.06\n",
      "| epoch 2 |  iter 6341 / 9295 | time 498[s] | loss 2.07\n",
      "| epoch 2 |  iter 6361 / 9295 | time 499[s] | loss 2.07\n",
      "| epoch 2 |  iter 6381 / 9295 | time 500[s] | loss 2.04\n",
      "| epoch 2 |  iter 6401 / 9295 | time 500[s] | loss 2.07\n",
      "| epoch 2 |  iter 6421 / 9295 | time 501[s] | loss 2.03\n",
      "| epoch 2 |  iter 6441 / 9295 | time 502[s] | loss 2.04\n",
      "| epoch 2 |  iter 6461 / 9295 | time 502[s] | loss 2.06\n",
      "| epoch 2 |  iter 6481 / 9295 | time 503[s] | loss 2.08\n",
      "| epoch 2 |  iter 6501 / 9295 | time 503[s] | loss 2.05\n",
      "| epoch 2 |  iter 6521 / 9295 | time 504[s] | loss 2.09\n",
      "| epoch 2 |  iter 6541 / 9295 | time 505[s] | loss 2.06\n",
      "| epoch 2 |  iter 6561 / 9295 | time 505[s] | loss 2.06\n",
      "| epoch 2 |  iter 6581 / 9295 | time 506[s] | loss 2.05\n",
      "| epoch 2 |  iter 6601 / 9295 | time 507[s] | loss 2.07\n",
      "| epoch 2 |  iter 6621 / 9295 | time 507[s] | loss 2.03\n",
      "| epoch 2 |  iter 6641 / 9295 | time 508[s] | loss 2.04\n",
      "| epoch 2 |  iter 6661 / 9295 | time 509[s] | loss 2.00\n",
      "| epoch 2 |  iter 6681 / 9295 | time 509[s] | loss 2.06\n",
      "| epoch 2 |  iter 6701 / 9295 | time 510[s] | loss 2.06\n",
      "| epoch 2 |  iter 6721 / 9295 | time 510[s] | loss 2.05\n",
      "| epoch 2 |  iter 6741 / 9295 | time 511[s] | loss 2.07\n",
      "| epoch 2 |  iter 6761 / 9295 | time 512[s] | loss 2.02\n",
      "| epoch 2 |  iter 6781 / 9295 | time 512[s] | loss 2.07\n",
      "| epoch 2 |  iter 6801 / 9295 | time 513[s] | loss 2.05\n",
      "| epoch 2 |  iter 6821 / 9295 | time 514[s] | loss 2.06\n",
      "| epoch 2 |  iter 6841 / 9295 | time 514[s] | loss 2.04\n",
      "| epoch 2 |  iter 6861 / 9295 | time 515[s] | loss 2.06\n",
      "| epoch 2 |  iter 6881 / 9295 | time 516[s] | loss 2.05\n",
      "| epoch 2 |  iter 6901 / 9295 | time 516[s] | loss 2.03\n",
      "| epoch 2 |  iter 6921 / 9295 | time 517[s] | loss 2.05\n",
      "| epoch 2 |  iter 6941 / 9295 | time 517[s] | loss 2.06\n",
      "| epoch 2 |  iter 6961 / 9295 | time 518[s] | loss 2.06\n",
      "| epoch 2 |  iter 6981 / 9295 | time 519[s] | loss 2.06\n",
      "| epoch 2 |  iter 7001 / 9295 | time 519[s] | loss 2.06\n",
      "| epoch 2 |  iter 7021 / 9295 | time 520[s] | loss 2.06\n",
      "| epoch 2 |  iter 7041 / 9295 | time 521[s] | loss 2.01\n",
      "| epoch 2 |  iter 7061 / 9295 | time 521[s] | loss 2.02\n",
      "| epoch 2 |  iter 7081 / 9295 | time 522[s] | loss 2.06\n",
      "| epoch 2 |  iter 7101 / 9295 | time 523[s] | loss 2.05\n",
      "| epoch 2 |  iter 7121 / 9295 | time 523[s] | loss 2.04\n",
      "| epoch 2 |  iter 7141 / 9295 | time 524[s] | loss 2.08\n",
      "| epoch 2 |  iter 7161 / 9295 | time 524[s] | loss 2.07\n",
      "| epoch 2 |  iter 7181 / 9295 | time 525[s] | loss 2.05\n",
      "| epoch 2 |  iter 7201 / 9295 | time 526[s] | loss 2.04\n",
      "| epoch 2 |  iter 7221 / 9295 | time 526[s] | loss 2.07\n",
      "| epoch 2 |  iter 7241 / 9295 | time 527[s] | loss 2.04\n",
      "| epoch 2 |  iter 7261 / 9295 | time 528[s] | loss 2.04\n",
      "| epoch 2 |  iter 7281 / 9295 | time 528[s] | loss 2.04\n",
      "| epoch 2 |  iter 7301 / 9295 | time 529[s] | loss 2.07\n",
      "| epoch 2 |  iter 7321 / 9295 | time 530[s] | loss 2.03\n",
      "| epoch 2 |  iter 7341 / 9295 | time 530[s] | loss 2.05\n",
      "| epoch 2 |  iter 7361 / 9295 | time 531[s] | loss 2.04\n",
      "| epoch 2 |  iter 7381 / 9295 | time 531[s] | loss 2.01\n",
      "| epoch 2 |  iter 7401 / 9295 | time 532[s] | loss 2.08\n",
      "| epoch 2 |  iter 7421 / 9295 | time 533[s] | loss 2.06\n",
      "| epoch 2 |  iter 7441 / 9295 | time 533[s] | loss 2.05\n",
      "| epoch 2 |  iter 7461 / 9295 | time 534[s] | loss 2.03\n",
      "| epoch 2 |  iter 7481 / 9295 | time 535[s] | loss 2.04\n",
      "| epoch 2 |  iter 7501 / 9295 | time 535[s] | loss 2.02\n",
      "| epoch 2 |  iter 7521 / 9295 | time 536[s] | loss 2.03\n",
      "| epoch 2 |  iter 7541 / 9295 | time 537[s] | loss 2.05\n",
      "| epoch 2 |  iter 7561 / 9295 | time 537[s] | loss 2.04\n",
      "| epoch 2 |  iter 7581 / 9295 | time 538[s] | loss 2.05\n",
      "| epoch 2 |  iter 7601 / 9295 | time 538[s] | loss 2.04\n",
      "| epoch 2 |  iter 7621 / 9295 | time 539[s] | loss 2.00\n",
      "| epoch 2 |  iter 7641 / 9295 | time 540[s] | loss 2.05\n",
      "| epoch 2 |  iter 7661 / 9295 | time 540[s] | loss 2.05\n",
      "| epoch 2 |  iter 7681 / 9295 | time 541[s] | loss 2.02\n",
      "| epoch 2 |  iter 7701 / 9295 | time 542[s] | loss 1.99\n",
      "| epoch 2 |  iter 7721 / 9295 | time 542[s] | loss 2.00\n",
      "| epoch 2 |  iter 7741 / 9295 | time 543[s] | loss 2.03\n",
      "| epoch 2 |  iter 7761 / 9295 | time 543[s] | loss 2.06\n",
      "| epoch 2 |  iter 7781 / 9295 | time 544[s] | loss 2.05\n",
      "| epoch 2 |  iter 7801 / 9295 | time 545[s] | loss 2.05\n",
      "| epoch 2 |  iter 7821 / 9295 | time 545[s] | loss 2.04\n",
      "| epoch 2 |  iter 7841 / 9295 | time 546[s] | loss 2.02\n",
      "| epoch 2 |  iter 7861 / 9295 | time 547[s] | loss 2.04\n",
      "| epoch 2 |  iter 7881 / 9295 | time 547[s] | loss 2.06\n",
      "| epoch 2 |  iter 7901 / 9295 | time 548[s] | loss 2.05\n",
      "| epoch 2 |  iter 7921 / 9295 | time 549[s] | loss 2.03\n",
      "| epoch 2 |  iter 7941 / 9295 | time 549[s] | loss 2.06\n",
      "| epoch 2 |  iter 7961 / 9295 | time 550[s] | loss 2.04\n",
      "| epoch 2 |  iter 7981 / 9295 | time 550[s] | loss 2.01\n",
      "| epoch 2 |  iter 8001 / 9295 | time 551[s] | loss 1.97\n",
      "| epoch 2 |  iter 8021 / 9295 | time 552[s] | loss 2.04\n",
      "| epoch 2 |  iter 8041 / 9295 | time 552[s] | loss 2.07\n",
      "| epoch 2 |  iter 8061 / 9295 | time 553[s] | loss 2.02\n",
      "| epoch 2 |  iter 8081 / 9295 | time 554[s] | loss 2.05\n",
      "| epoch 2 |  iter 8101 / 9295 | time 554[s] | loss 2.04\n",
      "| epoch 2 |  iter 8121 / 9295 | time 555[s] | loss 2.02\n",
      "| epoch 2 |  iter 8141 / 9295 | time 556[s] | loss 2.01\n",
      "| epoch 2 |  iter 8161 / 9295 | time 556[s] | loss 2.04\n",
      "| epoch 2 |  iter 8181 / 9295 | time 557[s] | loss 2.01\n",
      "| epoch 2 |  iter 8201 / 9295 | time 557[s] | loss 2.02\n",
      "| epoch 2 |  iter 8221 / 9295 | time 558[s] | loss 2.08\n",
      "| epoch 2 |  iter 8241 / 9295 | time 559[s] | loss 2.01\n",
      "| epoch 2 |  iter 8261 / 9295 | time 559[s] | loss 2.04\n",
      "| epoch 2 |  iter 8281 / 9295 | time 560[s] | loss 2.05\n",
      "| epoch 2 |  iter 8301 / 9295 | time 561[s] | loss 2.04\n",
      "| epoch 2 |  iter 8321 / 9295 | time 561[s] | loss 2.01\n",
      "| epoch 2 |  iter 8341 / 9295 | time 562[s] | loss 1.99\n",
      "| epoch 2 |  iter 8361 / 9295 | time 563[s] | loss 2.03\n",
      "| epoch 2 |  iter 8381 / 9295 | time 563[s] | loss 2.02\n",
      "| epoch 2 |  iter 8401 / 9295 | time 564[s] | loss 2.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 2 |  iter 8421 / 9295 | time 564[s] | loss 2.05\n",
      "| epoch 2 |  iter 8441 / 9295 | time 565[s] | loss 2.08\n",
      "| epoch 2 |  iter 8461 / 9295 | time 566[s] | loss 2.02\n",
      "| epoch 2 |  iter 8481 / 9295 | time 566[s] | loss 1.99\n",
      "| epoch 2 |  iter 8501 / 9295 | time 567[s] | loss 2.05\n",
      "| epoch 2 |  iter 8521 / 9295 | time 568[s] | loss 2.00\n",
      "| epoch 2 |  iter 8541 / 9295 | time 568[s] | loss 2.00\n",
      "| epoch 2 |  iter 8561 / 9295 | time 569[s] | loss 1.99\n",
      "| epoch 2 |  iter 8581 / 9295 | time 570[s] | loss 2.00\n",
      "| epoch 2 |  iter 8601 / 9295 | time 570[s] | loss 2.04\n",
      "| epoch 2 |  iter 8621 / 9295 | time 571[s] | loss 2.07\n",
      "| epoch 2 |  iter 8641 / 9295 | time 571[s] | loss 2.03\n",
      "| epoch 2 |  iter 8661 / 9295 | time 572[s] | loss 2.00\n",
      "| epoch 2 |  iter 8681 / 9295 | time 573[s] | loss 2.00\n",
      "| epoch 2 |  iter 8701 / 9295 | time 573[s] | loss 2.02\n",
      "| epoch 2 |  iter 8721 / 9295 | time 574[s] | loss 1.98\n",
      "| epoch 2 |  iter 8741 / 9295 | time 575[s] | loss 2.01\n",
      "| epoch 2 |  iter 8761 / 9295 | time 575[s] | loss 2.06\n",
      "| epoch 2 |  iter 8781 / 9295 | time 576[s] | loss 2.02\n",
      "| epoch 2 |  iter 8801 / 9295 | time 577[s] | loss 2.07\n",
      "| epoch 2 |  iter 8821 / 9295 | time 577[s] | loss 2.01\n",
      "| epoch 2 |  iter 8841 / 9295 | time 578[s] | loss 2.08\n",
      "| epoch 2 |  iter 8861 / 9295 | time 578[s] | loss 2.01\n",
      "| epoch 2 |  iter 8881 / 9295 | time 579[s] | loss 2.03\n",
      "| epoch 2 |  iter 8901 / 9295 | time 580[s] | loss 2.01\n",
      "| epoch 2 |  iter 8921 / 9295 | time 580[s] | loss 2.01\n",
      "| epoch 2 |  iter 8941 / 9295 | time 581[s] | loss 1.99\n",
      "| epoch 2 |  iter 8961 / 9295 | time 582[s] | loss 2.02\n",
      "| epoch 2 |  iter 8981 / 9295 | time 582[s] | loss 2.02\n",
      "| epoch 2 |  iter 9001 / 9295 | time 583[s] | loss 2.03\n",
      "| epoch 2 |  iter 9021 / 9295 | time 584[s] | loss 2.06\n",
      "| epoch 2 |  iter 9041 / 9295 | time 584[s] | loss 2.04\n",
      "| epoch 2 |  iter 9061 / 9295 | time 585[s] | loss 1.99\n",
      "| epoch 2 |  iter 9081 / 9295 | time 585[s] | loss 2.03\n",
      "| epoch 2 |  iter 9101 / 9295 | time 586[s] | loss 2.05\n",
      "| epoch 2 |  iter 9121 / 9295 | time 587[s] | loss 2.01\n",
      "| epoch 2 |  iter 9141 / 9295 | time 587[s] | loss 2.02\n",
      "| epoch 2 |  iter 9161 / 9295 | time 588[s] | loss 2.03\n",
      "| epoch 2 |  iter 9181 / 9295 | time 589[s] | loss 2.05\n",
      "| epoch 2 |  iter 9201 / 9295 | time 589[s] | loss 2.02\n",
      "| epoch 2 |  iter 9221 / 9295 | time 590[s] | loss 2.00\n",
      "| epoch 2 |  iter 9241 / 9295 | time 591[s] | loss 2.04\n",
      "| epoch 2 |  iter 9261 / 9295 | time 591[s] | loss 2.01\n",
      "| epoch 2 |  iter 9281 / 9295 | time 592[s] | loss 2.07\n",
      "| epoch 3 |  iter 1 / 9295 | time 592[s] | loss 2.00\n",
      "| epoch 3 |  iter 21 / 9295 | time 593[s] | loss 1.95\n",
      "| epoch 3 |  iter 41 / 9295 | time 594[s] | loss 1.94\n",
      "| epoch 3 |  iter 61 / 9295 | time 594[s] | loss 1.96\n",
      "| epoch 3 |  iter 81 / 9295 | time 595[s] | loss 1.97\n",
      "| epoch 3 |  iter 101 / 9295 | time 596[s] | loss 1.95\n",
      "| epoch 3 |  iter 121 / 9295 | time 596[s] | loss 1.96\n",
      "| epoch 3 |  iter 141 / 9295 | time 597[s] | loss 1.92\n",
      "| epoch 3 |  iter 161 / 9295 | time 597[s] | loss 1.96\n",
      "| epoch 3 |  iter 181 / 9295 | time 598[s] | loss 1.95\n",
      "| epoch 3 |  iter 201 / 9295 | time 599[s] | loss 1.96\n",
      "| epoch 3 |  iter 221 / 9295 | time 599[s] | loss 1.96\n",
      "| epoch 3 |  iter 241 / 9295 | time 600[s] | loss 1.97\n",
      "| epoch 3 |  iter 261 / 9295 | time 601[s] | loss 1.98\n",
      "| epoch 3 |  iter 281 / 9295 | time 601[s] | loss 1.96\n",
      "| epoch 3 |  iter 301 / 9295 | time 602[s] | loss 1.94\n",
      "| epoch 3 |  iter 321 / 9295 | time 603[s] | loss 1.92\n",
      "| epoch 3 |  iter 341 / 9295 | time 603[s] | loss 1.92\n",
      "| epoch 3 |  iter 361 / 9295 | time 604[s] | loss 1.93\n",
      "| epoch 3 |  iter 381 / 9295 | time 604[s] | loss 1.98\n",
      "| epoch 3 |  iter 401 / 9295 | time 605[s] | loss 1.98\n",
      "| epoch 3 |  iter 421 / 9295 | time 606[s] | loss 1.96\n",
      "| epoch 3 |  iter 441 / 9295 | time 606[s] | loss 1.93\n",
      "| epoch 3 |  iter 461 / 9295 | time 607[s] | loss 1.94\n",
      "| epoch 3 |  iter 481 / 9295 | time 608[s] | loss 1.94\n",
      "| epoch 3 |  iter 501 / 9295 | time 608[s] | loss 1.95\n",
      "| epoch 3 |  iter 521 / 9295 | time 609[s] | loss 1.95\n",
      "| epoch 3 |  iter 541 / 9295 | time 609[s] | loss 1.91\n",
      "| epoch 3 |  iter 561 / 9295 | time 610[s] | loss 1.95\n",
      "| epoch 3 |  iter 581 / 9295 | time 611[s] | loss 1.97\n",
      "| epoch 3 |  iter 601 / 9295 | time 611[s] | loss 1.96\n",
      "| epoch 3 |  iter 621 / 9295 | time 612[s] | loss 1.96\n",
      "| epoch 3 |  iter 641 / 9295 | time 613[s] | loss 1.99\n",
      "| epoch 3 |  iter 661 / 9295 | time 613[s] | loss 1.93\n",
      "| epoch 3 |  iter 681 / 9295 | time 614[s] | loss 1.93\n",
      "| epoch 3 |  iter 701 / 9295 | time 615[s] | loss 1.95\n",
      "| epoch 3 |  iter 721 / 9295 | time 615[s] | loss 2.00\n",
      "| epoch 3 |  iter 741 / 9295 | time 616[s] | loss 1.98\n",
      "| epoch 3 |  iter 761 / 9295 | time 616[s] | loss 1.96\n",
      "| epoch 3 |  iter 781 / 9295 | time 617[s] | loss 1.91\n",
      "| epoch 3 |  iter 801 / 9295 | time 618[s] | loss 1.96\n",
      "| epoch 3 |  iter 821 / 9295 | time 618[s] | loss 1.94\n",
      "| epoch 3 |  iter 841 / 9295 | time 619[s] | loss 1.97\n",
      "| epoch 3 |  iter 861 / 9295 | time 620[s] | loss 1.97\n",
      "| epoch 3 |  iter 881 / 9295 | time 620[s] | loss 1.92\n",
      "| epoch 3 |  iter 901 / 9295 | time 621[s] | loss 1.97\n",
      "| epoch 3 |  iter 921 / 9295 | time 622[s] | loss 1.99\n",
      "| epoch 3 |  iter 941 / 9295 | time 622[s] | loss 1.95\n",
      "| epoch 3 |  iter 961 / 9295 | time 623[s] | loss 1.95\n",
      "| epoch 3 |  iter 981 / 9295 | time 623[s] | loss 1.93\n",
      "| epoch 3 |  iter 1001 / 9295 | time 624[s] | loss 1.97\n",
      "| epoch 3 |  iter 1021 / 9295 | time 625[s] | loss 1.97\n",
      "| epoch 3 |  iter 1041 / 9295 | time 625[s] | loss 1.95\n",
      "| epoch 3 |  iter 1061 / 9295 | time 626[s] | loss 1.97\n",
      "| epoch 3 |  iter 1081 / 9295 | time 627[s] | loss 1.94\n",
      "| epoch 3 |  iter 1101 / 9295 | time 627[s] | loss 1.94\n",
      "| epoch 3 |  iter 1121 / 9295 | time 628[s] | loss 1.92\n",
      "| epoch 3 |  iter 1141 / 9295 | time 628[s] | loss 1.93\n",
      "| epoch 3 |  iter 1161 / 9295 | time 629[s] | loss 1.93\n",
      "| epoch 3 |  iter 1181 / 9295 | time 630[s] | loss 1.94\n",
      "| epoch 3 |  iter 1201 / 9295 | time 630[s] | loss 1.94\n",
      "| epoch 3 |  iter 1221 / 9295 | time 631[s] | loss 1.94\n",
      "| epoch 3 |  iter 1241 / 9295 | time 632[s] | loss 1.88\n",
      "| epoch 3 |  iter 1261 / 9295 | time 632[s] | loss 1.96\n",
      "| epoch 3 |  iter 1281 / 9295 | time 633[s] | loss 1.93\n",
      "| epoch 3 |  iter 1301 / 9295 | time 634[s] | loss 1.94\n",
      "| epoch 3 |  iter 1321 / 9295 | time 634[s] | loss 1.97\n",
      "| epoch 3 |  iter 1341 / 9295 | time 635[s] | loss 1.93\n",
      "| epoch 3 |  iter 1361 / 9295 | time 635[s] | loss 1.93\n",
      "| epoch 3 |  iter 1381 / 9295 | time 636[s] | loss 1.97\n",
      "| epoch 3 |  iter 1401 / 9295 | time 637[s] | loss 1.90\n",
      "| epoch 3 |  iter 1421 / 9295 | time 637[s] | loss 1.98\n",
      "| epoch 3 |  iter 1441 / 9295 | time 638[s] | loss 1.99\n",
      "| epoch 3 |  iter 1461 / 9295 | time 639[s] | loss 1.92\n",
      "| epoch 3 |  iter 1481 / 9295 | time 639[s] | loss 1.92\n",
      "| epoch 3 |  iter 1501 / 9295 | time 640[s] | loss 1.92\n",
      "| epoch 3 |  iter 1521 / 9295 | time 641[s] | loss 1.91\n",
      "| epoch 3 |  iter 1541 / 9295 | time 641[s] | loss 1.95\n",
      "| epoch 3 |  iter 1561 / 9295 | time 642[s] | loss 1.94\n",
      "| epoch 3 |  iter 1581 / 9295 | time 642[s] | loss 1.95\n",
      "| epoch 3 |  iter 1601 / 9295 | time 643[s] | loss 1.94\n",
      "| epoch 3 |  iter 1621 / 9295 | time 644[s] | loss 1.97\n",
      "| epoch 3 |  iter 1641 / 9295 | time 644[s] | loss 1.94\n",
      "| epoch 3 |  iter 1661 / 9295 | time 645[s] | loss 1.97\n",
      "| epoch 3 |  iter 1681 / 9295 | time 646[s] | loss 1.95\n",
      "| epoch 3 |  iter 1701 / 9295 | time 646[s] | loss 1.93\n",
      "| epoch 3 |  iter 1721 / 9295 | time 647[s] | loss 1.94\n",
      "| epoch 3 |  iter 1741 / 9295 | time 648[s] | loss 1.92\n",
      "| epoch 3 |  iter 1761 / 9295 | time 648[s] | loss 1.96\n",
      "| epoch 3 |  iter 1781 / 9295 | time 649[s] | loss 1.98\n",
      "| epoch 3 |  iter 1801 / 9295 | time 649[s] | loss 1.99\n",
      "| epoch 3 |  iter 1821 / 9295 | time 650[s] | loss 1.94\n",
      "| epoch 3 |  iter 1841 / 9295 | time 651[s] | loss 1.98\n",
      "| epoch 3 |  iter 1861 / 9295 | time 651[s] | loss 1.99\n",
      "| epoch 3 |  iter 1881 / 9295 | time 652[s] | loss 1.94\n",
      "| epoch 3 |  iter 1901 / 9295 | time 653[s] | loss 1.94\n",
      "| epoch 3 |  iter 1921 / 9295 | time 653[s] | loss 1.95\n",
      "| epoch 3 |  iter 1941 / 9295 | time 654[s] | loss 1.97\n",
      "| epoch 3 |  iter 1961 / 9295 | time 655[s] | loss 1.92\n",
      "| epoch 3 |  iter 1981 / 9295 | time 655[s] | loss 1.97\n",
      "| epoch 3 |  iter 2001 / 9295 | time 656[s] | loss 1.96\n",
      "| epoch 3 |  iter 2021 / 9295 | time 656[s] | loss 1.96\n",
      "| epoch 3 |  iter 2041 / 9295 | time 657[s] | loss 1.91\n",
      "| epoch 3 |  iter 2061 / 9295 | time 658[s] | loss 1.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 3 |  iter 2081 / 9295 | time 658[s] | loss 1.99\n",
      "| epoch 3 |  iter 2101 / 9295 | time 659[s] | loss 1.92\n",
      "| epoch 3 |  iter 2121 / 9295 | time 660[s] | loss 1.94\n",
      "| epoch 3 |  iter 2141 / 9295 | time 660[s] | loss 1.96\n",
      "| epoch 3 |  iter 2161 / 9295 | time 661[s] | loss 1.94\n",
      "| epoch 3 |  iter 2181 / 9295 | time 662[s] | loss 1.96\n",
      "| epoch 3 |  iter 2201 / 9295 | time 662[s] | loss 1.93\n",
      "| epoch 3 |  iter 2221 / 9295 | time 667[s] | loss 1.94\n",
      "| epoch 3 |  iter 2241 / 9295 | time 668[s] | loss 1.90\n",
      "| epoch 3 |  iter 2261 / 9295 | time 668[s] | loss 1.97\n",
      "| epoch 3 |  iter 2281 / 9295 | time 669[s] | loss 1.95\n",
      "| epoch 3 |  iter 2301 / 9295 | time 670[s] | loss 1.92\n",
      "| epoch 3 |  iter 2321 / 9295 | time 670[s] | loss 1.96\n",
      "| epoch 3 |  iter 2341 / 9295 | time 671[s] | loss 1.99\n",
      "| epoch 3 |  iter 2361 / 9295 | time 672[s] | loss 1.94\n",
      "| epoch 3 |  iter 2381 / 9295 | time 672[s] | loss 1.94\n",
      "| epoch 3 |  iter 2401 / 9295 | time 673[s] | loss 1.95\n",
      "| epoch 3 |  iter 2421 / 9295 | time 674[s] | loss 1.93\n",
      "| epoch 3 |  iter 2441 / 9295 | time 674[s] | loss 1.96\n",
      "| epoch 3 |  iter 2461 / 9295 | time 675[s] | loss 1.95\n",
      "| epoch 3 |  iter 2481 / 9295 | time 675[s] | loss 1.93\n",
      "| epoch 3 |  iter 2501 / 9295 | time 676[s] | loss 1.96\n",
      "| epoch 3 |  iter 2521 / 9295 | time 677[s] | loss 1.92\n",
      "| epoch 3 |  iter 2541 / 9295 | time 677[s] | loss 1.92\n",
      "| epoch 3 |  iter 2561 / 9295 | time 678[s] | loss 1.96\n",
      "| epoch 3 |  iter 2581 / 9295 | time 679[s] | loss 1.94\n",
      "| epoch 3 |  iter 2601 / 9295 | time 679[s] | loss 1.93\n",
      "| epoch 3 |  iter 2621 / 9295 | time 680[s] | loss 1.94\n",
      "| epoch 3 |  iter 2641 / 9295 | time 681[s] | loss 1.93\n",
      "| epoch 3 |  iter 2661 / 9295 | time 681[s] | loss 1.92\n",
      "| epoch 3 |  iter 2681 / 9295 | time 682[s] | loss 1.95\n",
      "| epoch 3 |  iter 2701 / 9295 | time 683[s] | loss 1.92\n",
      "| epoch 3 |  iter 2721 / 9295 | time 684[s] | loss 1.94\n",
      "| epoch 3 |  iter 2741 / 9295 | time 684[s] | loss 1.96\n",
      "| epoch 3 |  iter 2761 / 9295 | time 685[s] | loss 1.95\n",
      "| epoch 3 |  iter 2781 / 9295 | time 686[s] | loss 1.97\n",
      "| epoch 3 |  iter 2801 / 9295 | time 686[s] | loss 1.86\n",
      "| epoch 3 |  iter 2821 / 9295 | time 687[s] | loss 1.92\n",
      "| epoch 3 |  iter 2841 / 9295 | time 688[s] | loss 1.93\n",
      "| epoch 3 |  iter 2861 / 9295 | time 688[s] | loss 1.92\n",
      "| epoch 3 |  iter 2881 / 9295 | time 689[s] | loss 1.97\n",
      "| epoch 3 |  iter 2901 / 9295 | time 689[s] | loss 1.92\n",
      "| epoch 3 |  iter 2921 / 9295 | time 690[s] | loss 1.94\n",
      "| epoch 3 |  iter 2941 / 9295 | time 691[s] | loss 1.97\n",
      "| epoch 3 |  iter 2961 / 9295 | time 691[s] | loss 1.95\n",
      "| epoch 3 |  iter 2981 / 9295 | time 692[s] | loss 1.94\n",
      "| epoch 3 |  iter 3001 / 9295 | time 693[s] | loss 1.95\n",
      "| epoch 3 |  iter 3021 / 9295 | time 693[s] | loss 1.88\n",
      "| epoch 3 |  iter 3041 / 9295 | time 694[s] | loss 1.93\n",
      "| epoch 3 |  iter 3061 / 9295 | time 694[s] | loss 1.98\n",
      "| epoch 3 |  iter 3081 / 9295 | time 695[s] | loss 1.96\n",
      "| epoch 3 |  iter 3101 / 9295 | time 696[s] | loss 1.95\n",
      "| epoch 3 |  iter 3121 / 9295 | time 696[s] | loss 1.93\n",
      "| epoch 3 |  iter 3141 / 9295 | time 697[s] | loss 1.94\n",
      "| epoch 3 |  iter 3161 / 9295 | time 698[s] | loss 1.94\n",
      "| epoch 3 |  iter 3181 / 9295 | time 698[s] | loss 1.91\n",
      "| epoch 3 |  iter 3201 / 9295 | time 699[s] | loss 1.91\n",
      "| epoch 3 |  iter 3221 / 9295 | time 700[s] | loss 1.97\n",
      "| epoch 3 |  iter 3241 / 9295 | time 700[s] | loss 1.94\n",
      "| epoch 3 |  iter 3261 / 9295 | time 701[s] | loss 1.94\n",
      "| epoch 3 |  iter 3281 / 9295 | time 702[s] | loss 1.93\n",
      "| epoch 3 |  iter 3301 / 9295 | time 702[s] | loss 1.97\n",
      "| epoch 3 |  iter 3321 / 9295 | time 703[s] | loss 1.89\n",
      "| epoch 3 |  iter 3341 / 9295 | time 704[s] | loss 1.90\n",
      "| epoch 3 |  iter 3361 / 9295 | time 704[s] | loss 1.90\n",
      "| epoch 3 |  iter 3381 / 9295 | time 705[s] | loss 1.91\n",
      "| epoch 3 |  iter 3401 / 9295 | time 705[s] | loss 1.94\n",
      "| epoch 3 |  iter 3421 / 9295 | time 706[s] | loss 1.98\n",
      "| epoch 3 |  iter 3441 / 9295 | time 707[s] | loss 1.90\n",
      "| epoch 3 |  iter 3461 / 9295 | time 707[s] | loss 1.92\n",
      "| epoch 3 |  iter 3481 / 9295 | time 708[s] | loss 1.92\n",
      "| epoch 3 |  iter 3501 / 9295 | time 709[s] | loss 1.94\n",
      "| epoch 3 |  iter 3521 / 9295 | time 709[s] | loss 1.93\n",
      "| epoch 3 |  iter 3541 / 9295 | time 710[s] | loss 1.94\n",
      "| epoch 3 |  iter 3561 / 9295 | time 710[s] | loss 1.93\n",
      "| epoch 3 |  iter 3581 / 9295 | time 711[s] | loss 1.95\n",
      "| epoch 3 |  iter 3601 / 9295 | time 712[s] | loss 1.88\n",
      "| epoch 3 |  iter 3621 / 9295 | time 712[s] | loss 1.97\n",
      "| epoch 3 |  iter 3641 / 9295 | time 713[s] | loss 1.92\n",
      "| epoch 3 |  iter 3661 / 9295 | time 714[s] | loss 1.91\n",
      "| epoch 3 |  iter 3681 / 9295 | time 714[s] | loss 1.92\n",
      "| epoch 3 |  iter 3701 / 9295 | time 715[s] | loss 1.95\n",
      "| epoch 3 |  iter 3721 / 9295 | time 716[s] | loss 1.91\n",
      "| epoch 3 |  iter 3741 / 9295 | time 716[s] | loss 1.90\n",
      "| epoch 3 |  iter 3761 / 9295 | time 717[s] | loss 1.96\n",
      "| epoch 3 |  iter 3781 / 9295 | time 717[s] | loss 1.91\n",
      "| epoch 3 |  iter 3801 / 9295 | time 718[s] | loss 1.94\n",
      "| epoch 3 |  iter 3821 / 9295 | time 719[s] | loss 1.93\n",
      "| epoch 3 |  iter 3841 / 9295 | time 719[s] | loss 1.93\n",
      "| epoch 3 |  iter 3861 / 9295 | time 720[s] | loss 1.95\n",
      "| epoch 3 |  iter 3881 / 9295 | time 721[s] | loss 1.91\n",
      "| epoch 3 |  iter 3901 / 9295 | time 721[s] | loss 1.94\n",
      "| epoch 3 |  iter 3921 / 9295 | time 722[s] | loss 1.98\n",
      "| epoch 3 |  iter 3941 / 9295 | time 723[s] | loss 1.94\n",
      "| epoch 3 |  iter 3961 / 9295 | time 723[s] | loss 1.95\n",
      "| epoch 3 |  iter 3981 / 9295 | time 724[s] | loss 1.95\n",
      "| epoch 3 |  iter 4001 / 9295 | time 724[s] | loss 1.94\n",
      "| epoch 3 |  iter 4021 / 9295 | time 725[s] | loss 1.91\n",
      "| epoch 3 |  iter 4041 / 9295 | time 726[s] | loss 1.92\n",
      "| epoch 3 |  iter 4061 / 9295 | time 726[s] | loss 1.94\n",
      "| epoch 3 |  iter 4081 / 9295 | time 727[s] | loss 1.93\n",
      "| epoch 3 |  iter 4101 / 9295 | time 728[s] | loss 1.95\n",
      "| epoch 3 |  iter 4121 / 9295 | time 728[s] | loss 1.91\n",
      "| epoch 3 |  iter 4141 / 9295 | time 729[s] | loss 1.95\n",
      "| epoch 3 |  iter 4161 / 9295 | time 730[s] | loss 1.93\n",
      "| epoch 3 |  iter 4181 / 9295 | time 730[s] | loss 1.96\n",
      "| epoch 3 |  iter 4201 / 9295 | time 731[s] | loss 1.93\n",
      "| epoch 3 |  iter 4221 / 9295 | time 731[s] | loss 1.90\n",
      "| epoch 3 |  iter 4241 / 9295 | time 732[s] | loss 1.90\n",
      "| epoch 3 |  iter 4261 / 9295 | time 733[s] | loss 1.93\n",
      "| epoch 3 |  iter 4281 / 9295 | time 733[s] | loss 1.91\n",
      "| epoch 3 |  iter 4301 / 9295 | time 734[s] | loss 1.99\n",
      "| epoch 3 |  iter 4321 / 9295 | time 735[s] | loss 1.89\n",
      "| epoch 3 |  iter 4341 / 9295 | time 735[s] | loss 1.92\n",
      "| epoch 3 |  iter 4361 / 9295 | time 736[s] | loss 1.93\n",
      "| epoch 3 |  iter 4381 / 9295 | time 737[s] | loss 1.93\n",
      "| epoch 3 |  iter 4401 / 9295 | time 737[s] | loss 1.95\n",
      "| epoch 3 |  iter 4421 / 9295 | time 738[s] | loss 1.87\n",
      "| epoch 3 |  iter 4441 / 9295 | time 738[s] | loss 1.95\n",
      "| epoch 3 |  iter 4461 / 9295 | time 739[s] | loss 1.91\n",
      "| epoch 3 |  iter 4481 / 9295 | time 740[s] | loss 1.88\n",
      "| epoch 3 |  iter 4501 / 9295 | time 740[s] | loss 1.95\n",
      "| epoch 3 |  iter 4521 / 9295 | time 741[s] | loss 1.90\n",
      "| epoch 3 |  iter 4541 / 9295 | time 742[s] | loss 1.92\n",
      "| epoch 3 |  iter 4561 / 9295 | time 742[s] | loss 1.94\n",
      "| epoch 3 |  iter 4581 / 9295 | time 743[s] | loss 1.94\n",
      "| epoch 3 |  iter 4601 / 9295 | time 744[s] | loss 1.90\n",
      "| epoch 3 |  iter 4621 / 9295 | time 744[s] | loss 1.88\n",
      "| epoch 3 |  iter 4641 / 9295 | time 745[s] | loss 1.92\n",
      "| epoch 3 |  iter 4661 / 9295 | time 745[s] | loss 1.91\n",
      "| epoch 3 |  iter 4681 / 9295 | time 746[s] | loss 1.90\n",
      "| epoch 3 |  iter 4701 / 9295 | time 747[s] | loss 1.90\n",
      "| epoch 3 |  iter 4721 / 9295 | time 747[s] | loss 1.96\n",
      "| epoch 3 |  iter 4741 / 9295 | time 748[s] | loss 1.92\n",
      "| epoch 3 |  iter 4761 / 9295 | time 749[s] | loss 1.93\n",
      "| epoch 3 |  iter 4781 / 9295 | time 749[s] | loss 1.90\n",
      "| epoch 3 |  iter 4801 / 9295 | time 750[s] | loss 1.92\n",
      "| epoch 3 |  iter 4821 / 9295 | time 750[s] | loss 1.91\n",
      "| epoch 3 |  iter 4841 / 9295 | time 751[s] | loss 1.95\n",
      "| epoch 3 |  iter 4861 / 9295 | time 752[s] | loss 1.93\n",
      "| epoch 3 |  iter 4881 / 9295 | time 752[s] | loss 1.92\n",
      "| epoch 3 |  iter 4901 / 9295 | time 753[s] | loss 1.92\n",
      "| epoch 3 |  iter 4921 / 9295 | time 754[s] | loss 1.92\n",
      "| epoch 3 |  iter 4941 / 9295 | time 754[s] | loss 1.92\n",
      "| epoch 3 |  iter 4961 / 9295 | time 755[s] | loss 1.94\n",
      "| epoch 3 |  iter 4981 / 9295 | time 756[s] | loss 1.90\n",
      "| epoch 3 |  iter 5001 / 9295 | time 756[s] | loss 1.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 3 |  iter 5021 / 9295 | time 757[s] | loss 1.96\n",
      "| epoch 3 |  iter 5041 / 9295 | time 757[s] | loss 1.93\n",
      "| epoch 3 |  iter 5061 / 9295 | time 758[s] | loss 1.91\n",
      "| epoch 3 |  iter 5081 / 9295 | time 759[s] | loss 1.91\n",
      "| epoch 3 |  iter 5101 / 9295 | time 759[s] | loss 1.91\n",
      "| epoch 3 |  iter 5121 / 9295 | time 760[s] | loss 1.93\n",
      "| epoch 3 |  iter 5141 / 9295 | time 761[s] | loss 1.93\n",
      "| epoch 3 |  iter 5161 / 9295 | time 761[s] | loss 1.91\n",
      "| epoch 3 |  iter 5181 / 9295 | time 762[s] | loss 1.90\n",
      "| epoch 3 |  iter 5201 / 9295 | time 763[s] | loss 1.93\n",
      "| epoch 3 |  iter 5221 / 9295 | time 763[s] | loss 1.91\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common import config\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from cbow import CBOW\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "    \n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "tainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs']  =word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id \n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "with open(pkl_file, 'wb')as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ch5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, wh, b]\n",
    "        self.grads = [np.zeros_like(Wx),np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "    def forward(self, x, h_prev):\n",
    "            Wx, Wh, b = self.params\n",
    "            t = np.dot(h_prev, Wh) + np.dot(x, Wx) +b\n",
    "            h_next = np.tanh(t)\n",
    "            \n",
    "            self.cache = (x, h_prev, h_next)\n",
    "            return h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next **2)\n",
    "        db = np.sum(dt, axis = 0)\n",
    "        dwh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dwx = np.dot(x.t, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, sateful = False):\n",
    "            self.params = [Wx, Wh, b]\n",
    "            self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "            self.layers = None\n",
    "            self.h, self.dh = None, None\n",
    "            self.sateful = stateful\n",
    "            \n",
    "    def set_state(self, h):\n",
    "            self.h = h\n",
    "            \n",
    "    def reset_state(self):\n",
    "            self.h = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, xs):\n",
    "    Wx, Wh, b = self.params\n",
    "    N, T, D = xs.shape\n",
    "    D, H = Wx.shape\n",
    "    \n",
    "    self.layers = []\n",
    "    hs = np.empty((N, T, H), dtype = 'f')\n",
    "    \n",
    "    if not self.stateful or self.h is None:\n",
    "        self.h = np.zeros((N,H), dtype = 'f')\n",
    "        \n",
    "    for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t,:], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D,H = Wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T,D), dtype ='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "                layer = self.layers[t]\n",
    "                dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "                dx[:, t, :] = dx\n",
    "                for i, grad in enumerate(layer.grads):\n",
    "                        grads[i] += grad\n",
    "        for i ,grad in enumerate(grads):\n",
    "                self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.time_layers import *\n",
    "\n",
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randin\n",
    "        \n",
    "        # \n",
    "        embed_W = (rn(V,D)/ 100).astype('f')\n",
    "        rnn_Wx = (rn(D,H)/ np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H,H)/ np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H,V)/ np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # \n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "        \n",
    "        # t\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in self.layers:\n",
    "                self.params += layer.params\n",
    "                self.grads += layer.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "def backward(self, dout = 1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "                dout = layer.backward(dout)\n",
    "        return dout\n",
    "def reset_state(self):\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 1000, vocabulary size: 418\n",
      "| epoch 1 | perplexity 398.30\n",
      "| epoch 2 | perplexity 274.02\n",
      "| epoch 3 | perplexity 224.20\n",
      "| epoch 4 | perplexity 216.56\n",
      "| epoch 5 | perplexity 206.69\n",
      "| epoch 6 | perplexity 203.28\n",
      "| epoch 7 | perplexity 197.89\n",
      "| epoch 8 | perplexity 196.38\n",
      "| epoch 9 | perplexity 191.93\n",
      "| epoch 10 | perplexity 192.86\n",
      "| epoch 11 | perplexity 188.37\n",
      "| epoch 12 | perplexity 191.77\n",
      "| epoch 13 | perplexity 189.48\n",
      "| epoch 14 | perplexity 189.13\n",
      "| epoch 15 | perplexity 188.31\n",
      "| epoch 16 | perplexity 184.48\n",
      "| epoch 17 | perplexity 182.71\n",
      "| epoch 18 | perplexity 179.65\n",
      "| epoch 19 | perplexity 180.08\n",
      "| epoch 20 | perplexity 181.39\n",
      "| epoch 21 | perplexity 179.33\n",
      "| epoch 22 | perplexity 175.52\n",
      "| epoch 23 | perplexity 173.36\n",
      "| epoch 24 | perplexity 171.71\n",
      "| epoch 25 | perplexity 172.16\n",
      "| epoch 26 | perplexity 170.62\n",
      "| epoch 27 | perplexity 163.21\n",
      "| epoch 28 | perplexity 162.71\n",
      "| epoch 29 | perplexity 159.36\n",
      "| epoch 30 | perplexity 152.61\n",
      "| epoch 31 | perplexity 155.63\n",
      "| epoch 32 | perplexity 149.97\n",
      "| epoch 33 | perplexity 146.72\n",
      "| epoch 34 | perplexity 143.21\n",
      "| epoch 35 | perplexity 141.02\n",
      "| epoch 36 | perplexity 135.83\n",
      "| epoch 37 | perplexity 128.53\n",
      "| epoch 38 | perplexity 127.07\n",
      "| epoch 39 | perplexity 122.29\n",
      "| epoch 40 | perplexity 117.62\n",
      "| epoch 41 | perplexity 117.43\n",
      "| epoch 42 | perplexity 109.78\n",
      "| epoch 43 | perplexity 104.13\n",
      "| epoch 44 | perplexity 99.80\n",
      "| epoch 45 | perplexity 96.88\n",
      "| epoch 46 | perplexity 95.39\n",
      "| epoch 47 | perplexity 89.19\n",
      "| epoch 48 | perplexity 84.64\n",
      "| epoch 49 | perplexity 81.77\n",
      "| epoch 50 | perplexity 78.08\n",
      "| epoch 51 | perplexity 74.27\n",
      "| epoch 52 | perplexity 70.07\n",
      "| epoch 53 | perplexity 65.27\n",
      "| epoch 54 | perplexity 64.26\n",
      "| epoch 55 | perplexity 61.77\n",
      "| epoch 56 | perplexity 58.04\n",
      "| epoch 57 | perplexity 54.19\n",
      "| epoch 58 | perplexity 51.37\n",
      "| epoch 59 | perplexity 48.01\n",
      "| epoch 60 | perplexity 45.60\n",
      "| epoch 61 | perplexity 43.45\n",
      "| epoch 62 | perplexity 41.28\n",
      "| epoch 63 | perplexity 37.35\n",
      "| epoch 64 | perplexity 35.45\n",
      "| epoch 65 | perplexity 33.95\n",
      "| epoch 66 | perplexity 31.59\n",
      "| epoch 67 | perplexity 30.09\n",
      "| epoch 68 | perplexity 27.58\n",
      "| epoch 69 | perplexity 26.33\n",
      "| epoch 70 | perplexity 24.52\n",
      "| epoch 71 | perplexity 23.56\n",
      "| epoch 72 | perplexity 22.61\n",
      "| epoch 73 | perplexity 21.12\n",
      "| epoch 74 | perplexity 19.73\n",
      "| epoch 75 | perplexity 18.63\n",
      "| epoch 76 | perplexity 17.78\n",
      "| epoch 77 | perplexity 16.32\n",
      "| epoch 78 | perplexity 15.53\n",
      "| epoch 79 | perplexity 14.10\n",
      "| epoch 80 | perplexity 13.67\n",
      "| epoch 81 | perplexity 12.92\n",
      "| epoch 82 | perplexity 12.43\n",
      "| epoch 83 | perplexity 11.31\n",
      "| epoch 84 | perplexity 11.44\n",
      "| epoch 85 | perplexity 10.46\n",
      "| epoch 86 | perplexity 9.93\n",
      "| epoch 87 | perplexity 9.52\n",
      "| epoch 88 | perplexity 9.35\n",
      "| epoch 89 | perplexity 8.72\n",
      "| epoch 90 | perplexity 8.12\n",
      "| epoch 91 | perplexity 7.62\n",
      "| epoch 92 | perplexity 6.93\n",
      "| epoch 93 | perplexity 6.97\n",
      "| epoch 94 | perplexity 6.63\n",
      "| epoch 95 | perplexity 6.82\n",
      "| epoch 96 | perplexity 5.93\n",
      "| epoch 97 | perplexity 5.64\n",
      "| epoch 98 | perplexity 5.33\n",
      "| epoch 99 | perplexity 5.29\n",
      "| epoch 100 | perplexity 4.94\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.optimizer import SGD\n",
    "from dataset import ptb\n",
    "from simple_rnnlm import SimpleRnnlm\n",
    "\n",
    "\n",
    "# \n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 5  # Truncated BPTT\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# \n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]  # \n",
    "ts = corpus[1:]  # \n",
    "data_size = len(xs)\n",
    "print('corpus size: %d, vocabulary size: %d' % (corpus_size, vocab_size))\n",
    "\n",
    "# \n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "# \n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "# \n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # \n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # \n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n",
    "\n",
    "# \n",
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ch06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "            self.params = [Wx, Wh, b]\n",
    "            self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "            self.cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "        \n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "        \n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "        \n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "        \n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        self.cache - (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful = False):\n",
    "        self.params  =[Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def forward(self, xs):\n",
    "            Wx, Wh, b = self.params\n",
    "            N, T, D = xs.shape\n",
    "            H = Wh.shape[0]\n",
    "            \n",
    "            self.layers = []\n",
    "            hs = np.empty((N, T, H),dtype = 'f')\n",
    "            \n",
    "            if not self.stateful or self.h is None:\n",
    "                self.h = np.zeros((N, H), dtype = 'f')\n",
    "            if not self.stateful or self.c is None:\n",
    "                self.c = np.zeros((N, H), dtype = 'f')\n",
    "            \n",
    "            for t in range(T):\n",
    "                    layer  = LSTM(*self.params)\n",
    "                    self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "                    hs[:, t, :] = self.h\n",
    "                    \n",
    "                    self.layers.append(layer)\n",
    "                    \n",
    "            return hs\n",
    "        \n",
    "    def backward(self, dhs):\n",
    "            Wx, Wh, b = self.params\n",
    "            N, T, H = dhs.shape\n",
    "            D = Wx.shape[0]\n",
    "            \n",
    "            dxs = np.empty((N, T, D), dtype = 'f')\n",
    "            dh, dc = 0,0\n",
    "            \n",
    "            grads = [0, 0, 0]\n",
    "            for t in reversed(range(T)):\n",
    "                layer = self.layers[t]\n",
    "                dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "                dxs[:, t, :] = dx\n",
    "                for i, grad in enumerate(layer.grads):\n",
    "                    grads[i] += grad\n",
    "            for i, grad in enumerate(grads):\n",
    "                self.grads[i][...] = grad\n",
    "                self.dh = dh\n",
    "                return dxs\n",
    "            \n",
    "    def set_state(self, h, c =None):\n",
    "                self.h, self.c = h,c\n",
    "        \n",
    "    def reset_state(self):\n",
    "                self.h, self.c  =None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "import pickle\n",
    "\n",
    "class Rnnlm:\n",
    "    def __init__(self, vocab_size = 10000, wordvec_size = 100, hidden_size = 100):\n",
    "            V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "            rn = np.rondom.randn\n",
    "            \n",
    "            # \n",
    "            embed_W = (rn(V,D) / 100).astype('f')\n",
    "            lstm_Wx = (rn(D, 4 * H)/ np.sqrt(D)).astype('f')\n",
    "            lstm_Wh = (rn(H, 4 * H)/ np.sqrt(H)).astype('f')\n",
    "            lstm_b = np.zeros(4 * H).astype('f')\n",
    "            affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "            affine_b = np.zeros(V).astype('f')\n",
    "            \n",
    "            self.layers = [\n",
    "                TimeEmbedding(embed_W),\n",
    "                TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful = True),\n",
    "                TimeAffine(affine_W, affine_b)\n",
    "            ]\n",
    "            self.loss_layer = TimeSoftbaxWithLoss()\n",
    "            self.lstm_layer = self.layers[1]\n",
    "            \n",
    "            # \n",
    "            self.params, self.grads = [], []\n",
    "            for layer in self.layers:\n",
    "                    self.params += layer.params\n",
    "                    self.grads += layer.grads\n",
    "    def predict(self, xs):\n",
    "            for layer in self.layers:\n",
    "                xs = layer.forward(xs)\n",
    "            return xs\n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "            score = self.predict(xs)\n",
    "            loss = self.loss_layer.forward(score, ts)\n",
    "            return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "            dout = self.loss_layer.backward(dout)\n",
    "            for layer in reversed(seld.layers):\n",
    "                dout = layer.backward(dout)\n",
    "            return dout \n",
    "    \n",
    "    def reset_state(self):\n",
    "            self.lstm_layer.reset_state()\n",
    "            \n",
    "    def save_params(self, file_name = 'Rnnlm.pkl'):\n",
    "            with open(file_name, 'wb') as f :\n",
    "                pickle.dump(self.params, f)\n",
    "    \n",
    "    def load_params(self, file_name = 'Rnnlm.pkl'):\n",
    "            with open(file_name, 'rb') as f:\n",
    "                self.params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.np import *  # import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    '''\n",
    "     LSTM2Dropout\n",
    "     [1]weight tying[2][3]\n",
    "\n",
    "     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
    "     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
    "     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
    "    '''\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
    "                 hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ch07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ch06Rnnlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.functions import softmax\n",
    "from ch06.rnnlm import Rnnlm\n",
    "from ch06.better_rnnlm import BetterRnnlm\n",
    "\n",
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id, skip_id, skip_ids = None, sample_size = 100):\n",
    "            word_ids = [start_id]\n",
    "            \n",
    "            x = start_id\n",
    "            while len(word_size) < smaple_size:\n",
    "                x = np.array(x).reshape(1,1)\n",
    "                score = self.predict(x)\n",
    "                p = softmax(score.flatten())\n",
    "                \n",
    "                sampled  = np.random.choice(len(p), size = 1, p=p)\n",
    "                if(skip_ids is None) or (sampled not in skip_ids):\n",
    "                    x = sampled\n",
    "                    word_ids.append(int(x))\n",
    "            return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you fiscal hot-dipped fusion homes personal-computer department activity nervous mass-market blessing tackle metals electrical peripheral guests nationwide beers current processed broadway views norton electronics is girlfriend gubernatorial waertsilae cigarette dioxide dioxide applicants prerogatives reminded integrity violent bench necessity mount cox stress-related venerable three-month minerals pile investigated necessarily ken gathered restored interpret cautious intel soo now fixed-price loss bottom steelmaker status seasonally billing absence communities kick wrongdoing instrumental patrick dignity lawmakers how outsiders wheels o'connell views interpreted cathcart privilege dpc entitled treating windows restored besides engineers completely saying restriction hispanic salomon suitor peterson renewed witness natural-gas phrase trucks party peabody arteries\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from rnnlm_gen import RnnlmGen\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace('<eos>', '.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you had billed when the nation 's largest council failed to contain the transferred .\n",
      " an investment banking index posted the deal targets for ginnie mae losses for the debate seasonal positions .\n",
      " one of these victims bolstered by vast demands that is falling .\n",
      " he wo n't waive the european stock market .\n",
      " the proposals plan to crush union aviation european nations from several defendants and leaders to bolster marketing .\n",
      " analysts have run their stake and an violence order against their state and congress not mind .\n",
      " mr. roman took prove to make republican criminal joint petroleum 's\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from rnnlm_gen import RnnlmGen\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('../ch06/Rnnlm.pkl')\n",
    "\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace('<eos>', '.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.load_params('../ch06/Rnnlm.pkl')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can stop it.\n",
      " he also cited the stage of the city 's central bank barron 's short history in recent years with an appeal to illegal use in the hispanic agenda.\n",
      " the old crowd doing mccall is proud of what happened in the house of representatives.\n",
      " logic they attempts to sacrifice asia in washington then in the state as the future of the script.\n",
      " in the almost two years of japan 's largest metropolitan limits its own pursued image is supposed to be larger among the most sophisticated u.s. operations according to mr..\n",
      " no\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "from rnnlm_gen import BetterRnnlmGen\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "\n",
    "model = BetterRnnlmGen()\n",
    "model.load_params('../ch06/BetterRnnlm.pkl')\n",
    "\n",
    "# startskip\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "# \n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import sequence\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('addition.txt', seed=1984)\n",
    "cahr_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  0  2  0  0 11  5]\n",
      "[ 6  0 11  7  5]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(t_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71+118 \n",
      "_189 \n"
     ]
    }
   ],
   "source": [
    "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
    "print(''.join([id_to_char[c] for c in t_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V,D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embe_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful = False)\n",
    "        \n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[:, -1, :]\n",
    "def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "        \n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_w = (rn(V,D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_w = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful = True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "                self.params += layer.params\n",
    "                self.grads += layer.grads\n",
    "                \n",
    "    def forward(self, xs, h):\n",
    "            self.lstm.set_state(h)\n",
    "            \n",
    "            out = self.embed.forward(xs)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "            return score\n",
    "        \n",
    "    def backward(self, dscore):\n",
    "            dout = self.affine.backward(dscore)\n",
    "            dout = self.lstm.backward(dout)\n",
    "            dout = self.embed.backward(dout)\n",
    "            dh = self.lstm.dh\n",
    "            return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        samle_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape((1,1))\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            sample_id = mp.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "            \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V,D,H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V,D,H)\n",
    "        self.decoder = Decoder(V,D,H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "            decoder_xs, decoder_ts = ts[:, :-1], ts[:,1:]\n",
    "            \n",
    "            h = self.encoder.forward(xs)\n",
    "            score = self.decoder.forward(decoder_xs, h)\n",
    "            loss = self.softmax.forward(score, decoder_ts)\n",
    "            return loss\n",
    "    def backward(self, dout =1):\n",
    "            dout = self.softmax.backward(dout)\n",
    "            dh = self.decoder.backward(dout)\n",
    "            dout =self.encoder.backward(dh)\n",
    "            return dout\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "            h = self.encoder.forward(xs)\n",
    "            sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "            return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 2.56\n",
      "| epoch 1 |  iter 21 / 351 | time 0[s] | loss 2.53\n",
      "| epoch 1 |  iter 41 / 351 | time 0[s] | loss 2.17\n",
      "| epoch 1 |  iter 61 / 351 | time 1[s] | loss 1.96\n",
      "| epoch 1 |  iter 81 / 351 | time 1[s] | loss 1.92\n",
      "| epoch 1 |  iter 101 / 351 | time 2[s] | loss 1.87\n",
      "| epoch 1 |  iter 121 / 351 | time 2[s] | loss 1.85\n",
      "| epoch 1 |  iter 141 / 351 | time 3[s] | loss 1.83\n",
      "| epoch 1 |  iter 161 / 351 | time 3[s] | loss 1.79\n",
      "| epoch 1 |  iter 181 / 351 | time 4[s] | loss 1.77\n",
      "| epoch 1 |  iter 201 / 351 | time 4[s] | loss 1.77\n",
      "| epoch 1 |  iter 221 / 351 | time 5[s] | loss 1.76\n",
      "| epoch 1 |  iter 241 / 351 | time 5[s] | loss 1.76\n",
      "| epoch 1 |  iter 261 / 351 | time 6[s] | loss 1.76\n",
      "| epoch 1 |  iter 281 / 351 | time 6[s] | loss 1.75\n",
      "| epoch 1 |  iter 301 / 351 | time 6[s] | loss 1.74\n",
      "| epoch 1 |  iter 321 / 351 | time 7[s] | loss 1.75\n",
      "| epoch 1 |  iter 341 / 351 | time 7[s] | loss 1.74\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 100 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 100 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 100 \n",
      "---\n",
      "val acc 0.180%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.74\n",
      "| epoch 2 |  iter 21 / 351 | time 0[s] | loss 1.73\n",
      "| epoch 2 |  iter 41 / 351 | time 0[s] | loss 1.74\n",
      "| epoch 2 |  iter 61 / 351 | time 1[s] | loss 1.74\n",
      "| epoch 2 |  iter 81 / 351 | time 1[s] | loss 1.73\n",
      "| epoch 2 |  iter 101 / 351 | time 2[s] | loss 1.73\n",
      "| epoch 2 |  iter 121 / 351 | time 2[s] | loss 1.72\n",
      "| epoch 2 |  iter 141 / 351 | time 3[s] | loss 1.71\n",
      "| epoch 2 |  iter 161 / 351 | time 3[s] | loss 1.71\n",
      "| epoch 2 |  iter 181 / 351 | time 4[s] | loss 1.71\n",
      "| epoch 2 |  iter 201 / 351 | time 4[s] | loss 1.70\n",
      "| epoch 2 |  iter 221 / 351 | time 5[s] | loss 1.71\n",
      "| epoch 2 |  iter 241 / 351 | time 5[s] | loss 1.70\n",
      "| epoch 2 |  iter 261 / 351 | time 5[s] | loss 1.69\n",
      "| epoch 2 |  iter 281 / 351 | time 6[s] | loss 1.69\n",
      "| epoch 2 |  iter 301 / 351 | time 6[s] | loss 1.69\n",
      "| epoch 2 |  iter 321 / 351 | time 7[s] | loss 1.68\n",
      "| epoch 2 |  iter 341 / 351 | time 7[s] | loss 1.67\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 994 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 700 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 100 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 400 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1000\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1544\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 400 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 400 \n",
      "---\n",
      "val acc 0.220%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 1.66\n",
      "| epoch 3 |  iter 21 / 351 | time 0[s] | loss 1.66\n",
      "| epoch 3 |  iter 41 / 351 | time 1[s] | loss 1.65\n",
      "| epoch 3 |  iter 61 / 351 | time 1[s] | loss 1.63\n",
      "| epoch 3 |  iter 81 / 351 | time 2[s] | loss 1.62\n",
      "| epoch 3 |  iter 101 / 351 | time 2[s] | loss 1.62\n",
      "| epoch 3 |  iter 121 / 351 | time 3[s] | loss 1.60\n",
      "| epoch 3 |  iter 141 / 351 | time 3[s] | loss 1.59\n",
      "| epoch 3 |  iter 161 / 351 | time 4[s] | loss 1.57\n",
      "| epoch 3 |  iter 181 / 351 | time 4[s] | loss 1.57\n",
      "| epoch 3 |  iter 201 / 351 | time 5[s] | loss 1.56\n",
      "| epoch 3 |  iter 221 / 351 | time 5[s] | loss 1.54\n",
      "| epoch 3 |  iter 241 / 351 | time 6[s] | loss 1.52\n",
      "| epoch 3 |  iter 261 / 351 | time 6[s] | loss 1.52\n",
      "| epoch 3 |  iter 281 / 351 | time 7[s] | loss 1.52\n",
      "| epoch 3 |  iter 301 / 351 | time 7[s] | loss 1.50\n",
      "| epoch 3 |  iter 321 / 351 | time 8[s] | loss 1.49\n",
      "| epoch 3 |  iter 341 / 351 | time 8[s] | loss 1.48\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 108 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1001\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 648 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 138 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 448 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 848 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1011\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1373\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 868 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 348 \n",
      "---\n",
      "val acc 0.560%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 1.47\n",
      "| epoch 4 |  iter 21 / 351 | time 0[s] | loss 1.46\n",
      "| epoch 4 |  iter 41 / 351 | time 1[s] | loss 1.44\n",
      "| epoch 4 |  iter 61 / 351 | time 1[s] | loss 1.43\n",
      "| epoch 4 |  iter 81 / 351 | time 2[s] | loss 1.42\n",
      "| epoch 4 |  iter 101 / 351 | time 2[s] | loss 1.41\n",
      "| epoch 4 |  iter 121 / 351 | time 3[s] | loss 1.40\n",
      "| epoch 4 |  iter 141 / 351 | time 3[s] | loss 1.40\n",
      "| epoch 4 |  iter 161 / 351 | time 4[s] | loss 1.38\n",
      "| epoch 4 |  iter 181 / 351 | time 4[s] | loss 1.38\n",
      "| epoch 4 |  iter 201 / 351 | time 4[s] | loss 1.37\n",
      "| epoch 4 |  iter 221 / 351 | time 5[s] | loss 1.35\n",
      "| epoch 4 |  iter 241 / 351 | time 5[s] | loss 1.33\n",
      "| epoch 4 |  iter 261 / 351 | time 6[s] | loss 1.33\n",
      "| epoch 4 |  iter 281 / 351 | time 6[s] | loss 1.33\n",
      "| epoch 4 |  iter 301 / 351 | time 7[s] | loss 1.32\n",
      "| epoch 4 |  iter 321 / 351 | time 7[s] | loss 1.31\n",
      "| epoch 4 |  iter 341 / 351 | time 8[s] | loss 1.30\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 146 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1189\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[92m\u001b[0m 666 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 162 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 432 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 866 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1002\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1406\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 862 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 202 \n",
      "---\n",
      "val acc 1.060%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 1.28\n",
      "| epoch 5 |  iter 21 / 351 | time 0[s] | loss 1.29\n",
      "| epoch 5 |  iter 41 / 351 | time 0[s] | loss 1.28\n",
      "| epoch 5 |  iter 61 / 351 | time 1[s] | loss 1.27\n",
      "| epoch 5 |  iter 81 / 351 | time 1[s] | loss 1.27\n",
      "| epoch 5 |  iter 101 / 351 | time 2[s] | loss 1.26\n",
      "| epoch 5 |  iter 121 / 351 | time 2[s] | loss 1.26\n",
      "| epoch 5 |  iter 141 / 351 | time 3[s] | loss 1.27\n",
      "| epoch 5 |  iter 161 / 351 | time 3[s] | loss 1.26\n",
      "| epoch 5 |  iter 181 / 351 | time 4[s] | loss 1.25\n",
      "| epoch 5 |  iter 201 / 351 | time 4[s] | loss 1.23\n",
      "| epoch 5 |  iter 221 / 351 | time 4[s] | loss 1.22\n",
      "| epoch 5 |  iter 241 / 351 | time 5[s] | loss 1.21\n",
      "| epoch 5 |  iter 261 / 351 | time 5[s] | loss 1.21\n",
      "| epoch 5 |  iter 281 / 351 | time 6[s] | loss 1.21\n",
      "| epoch 5 |  iter 301 / 351 | time 6[s] | loss 1.20\n",
      "| epoch 5 |  iter 321 / 351 | time 7[s] | loss 1.19\n",
      "| epoch 5 |  iter 341 / 351 | time 7[s] | loss 1.18\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 145 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1168\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 665 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 192 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 431 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 895 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1015\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1493\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 891 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 221 \n",
      "---\n",
      "val acc 2.260%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 1.17\n",
      "| epoch 6 |  iter 21 / 351 | time 0[s] | loss 1.17\n",
      "| epoch 6 |  iter 41 / 351 | time 0[s] | loss 1.18\n",
      "| epoch 6 |  iter 61 / 351 | time 1[s] | loss 1.17\n",
      "| epoch 6 |  iter 81 / 351 | time 1[s] | loss 1.16\n",
      "| epoch 6 |  iter 101 / 351 | time 2[s] | loss 1.16\n",
      "| epoch 6 |  iter 121 / 351 | time 2[s] | loss 1.16\n",
      "| epoch 6 |  iter 141 / 351 | time 3[s] | loss 1.14\n",
      "| epoch 6 |  iter 161 / 351 | time 3[s] | loss 1.14\n",
      "| epoch 6 |  iter 181 / 351 | time 4[s] | loss 1.13\n",
      "| epoch 6 |  iter 201 / 351 | time 4[s] | loss 1.15\n",
      "| epoch 6 |  iter 221 / 351 | time 5[s] | loss 1.13\n",
      "| epoch 6 |  iter 241 / 351 | time 5[s] | loss 1.13\n",
      "| epoch 6 |  iter 261 / 351 | time 6[s] | loss 1.13\n",
      "| epoch 6 |  iter 281 / 351 | time 6[s] | loss 1.12\n",
      "| epoch 6 |  iter 301 / 351 | time 7[s] | loss 1.13\n",
      "| epoch 6 |  iter 321 / 351 | time 7[s] | loss 1.13\n",
      "| epoch 6 |  iter 341 / 351 | time 8[s] | loss 1.10\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 156 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1169\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 660 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 164 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 411 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 846 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1011\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1412\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 821 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 207 \n",
      "---\n",
      "val acc 2.120%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 1.12\n",
      "| epoch 7 |  iter 21 / 351 | time 0[s] | loss 1.09\n",
      "| epoch 7 |  iter 41 / 351 | time 0[s] | loss 1.09\n",
      "| epoch 7 |  iter 61 / 351 | time 1[s] | loss 1.09\n",
      "| epoch 7 |  iter 81 / 351 | time 2[s] | loss 1.08\n",
      "| epoch 7 |  iter 101 / 351 | time 2[s] | loss 1.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 7 |  iter 121 / 351 | time 3[s] | loss 1.08\n",
      "| epoch 7 |  iter 141 / 351 | time 4[s] | loss 1.08\n",
      "| epoch 7 |  iter 161 / 351 | time 4[s] | loss 1.08\n",
      "| epoch 7 |  iter 181 / 351 | time 5[s] | loss 1.09\n",
      "| epoch 7 |  iter 201 / 351 | time 6[s] | loss 1.08\n",
      "| epoch 7 |  iter 221 / 351 | time 6[s] | loss 1.07\n",
      "| epoch 7 |  iter 241 / 351 | time 7[s] | loss 1.05\n",
      "| epoch 7 |  iter 261 / 351 | time 7[s] | loss 1.05\n",
      "| epoch 7 |  iter 281 / 351 | time 8[s] | loss 1.06\n",
      "| epoch 7 |  iter 301 / 351 | time 8[s] | loss 1.06\n",
      "| epoch 7 |  iter 321 / 351 | time 9[s] | loss 1.06\n",
      "| epoch 7 |  iter 341 / 351 | time 9[s] | loss 1.05\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 156 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1166\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 665 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 161 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 407 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 893 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1107\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1444\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 227 \n",
      "---\n",
      "val acc 3.820%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 1.07\n",
      "| epoch 8 |  iter 21 / 351 | time 0[s] | loss 1.04\n",
      "| epoch 8 |  iter 41 / 351 | time 0[s] | loss 1.04\n",
      "| epoch 8 |  iter 61 / 351 | time 1[s] | loss 1.03\n",
      "| epoch 8 |  iter 81 / 351 | time 1[s] | loss 1.03\n",
      "| epoch 8 |  iter 101 / 351 | time 2[s] | loss 1.04\n",
      "| epoch 8 |  iter 121 / 351 | time 2[s] | loss 1.04\n",
      "| epoch 8 |  iter 141 / 351 | time 3[s] | loss 1.03\n",
      "| epoch 8 |  iter 161 / 351 | time 3[s] | loss 1.04\n",
      "| epoch 8 |  iter 181 / 351 | time 4[s] | loss 1.03\n",
      "| epoch 8 |  iter 201 / 351 | time 4[s] | loss 1.02\n",
      "| epoch 8 |  iter 221 / 351 | time 4[s] | loss 1.02\n",
      "| epoch 8 |  iter 241 / 351 | time 5[s] | loss 1.02\n",
      "| epoch 8 |  iter 261 / 351 | time 5[s] | loss 1.02\n",
      "| epoch 8 |  iter 281 / 351 | time 6[s] | loss 1.01\n",
      "| epoch 8 |  iter 301 / 351 | time 6[s] | loss 1.02\n",
      "| epoch 8 |  iter 321 / 351 | time 7[s] | loss 1.01\n",
      "| epoch 8 |  iter 341 / 351 | time 7[s] | loss 1.04\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 156 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1160\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 655 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 150 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 400 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 836 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1449\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 858 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 207 \n",
      "---\n",
      "val acc 2.480%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 1.03\n",
      "| epoch 9 |  iter 21 / 351 | time 0[s] | loss 1.05\n",
      "| epoch 9 |  iter 41 / 351 | time 0[s] | loss 1.02\n",
      "| epoch 9 |  iter 61 / 351 | time 1[s] | loss 1.01\n",
      "| epoch 9 |  iter 81 / 351 | time 1[s] | loss 1.00\n",
      "| epoch 9 |  iter 101 / 351 | time 2[s] | loss 0.99\n",
      "| epoch 9 |  iter 121 / 351 | time 2[s] | loss 1.00\n",
      "| epoch 9 |  iter 141 / 351 | time 3[s] | loss 1.01\n",
      "| epoch 9 |  iter 161 / 351 | time 3[s] | loss 0.98\n",
      "| epoch 9 |  iter 181 / 351 | time 4[s] | loss 0.99\n",
      "| epoch 9 |  iter 201 / 351 | time 4[s] | loss 0.98\n",
      "| epoch 9 |  iter 221 / 351 | time 4[s] | loss 0.99\n",
      "| epoch 9 |  iter 241 / 351 | time 5[s] | loss 0.98\n",
      "| epoch 9 |  iter 261 / 351 | time 5[s] | loss 0.99\n",
      "| epoch 9 |  iter 281 / 351 | time 6[s] | loss 0.98\n",
      "| epoch 9 |  iter 301 / 351 | time 6[s] | loss 0.98\n",
      "| epoch 9 |  iter 321 / 351 | time 7[s] | loss 0.98\n",
      "| epoch 9 |  iter 341 / 351 | time 7[s] | loss 0.97\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 167 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1128\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 665 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 172 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 418 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 846 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1428\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 852 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 228 \n",
      "---\n",
      "val acc 5.240%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.95\n",
      "| epoch 10 |  iter 21 / 351 | time 0[s] | loss 0.97\n",
      "| epoch 10 |  iter 41 / 351 | time 0[s] | loss 0.96\n",
      "| epoch 10 |  iter 61 / 351 | time 1[s] | loss 0.97\n",
      "| epoch 10 |  iter 81 / 351 | time 1[s] | loss 0.98\n",
      "| epoch 10 |  iter 101 / 351 | time 2[s] | loss 1.01\n",
      "| epoch 10 |  iter 121 / 351 | time 2[s] | loss 1.01\n",
      "| epoch 10 |  iter 141 / 351 | time 3[s] | loss 1.00\n",
      "| epoch 10 |  iter 161 / 351 | time 3[s] | loss 0.98\n",
      "| epoch 10 |  iter 181 / 351 | time 4[s] | loss 0.96\n",
      "| epoch 10 |  iter 201 / 351 | time 4[s] | loss 0.96\n",
      "| epoch 10 |  iter 221 / 351 | time 5[s] | loss 0.95\n",
      "| epoch 10 |  iter 241 / 351 | time 5[s] | loss 0.94\n",
      "| epoch 10 |  iter 261 / 351 | time 5[s] | loss 0.96\n",
      "| epoch 10 |  iter 281 / 351 | time 6[s] | loss 0.96\n",
      "| epoch 10 |  iter 301 / 351 | time 6[s] | loss 0.96\n",
      "| epoch 10 |  iter 321 / 351 | time 7[s] | loss 0.95\n",
      "| epoch 10 |  iter 341 / 351 | time 7[s] | loss 0.95\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 160 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1127\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 660 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 170 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 419 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 856 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1409\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 240 \n",
      "---\n",
      "val acc 5.780%\n",
      "| epoch 11 |  iter 1 / 351 | time 0[s] | loss 0.94\n",
      "| epoch 11 |  iter 21 / 351 | time 0[s] | loss 0.95\n",
      "| epoch 11 |  iter 41 / 351 | time 1[s] | loss 0.95\n",
      "| epoch 11 |  iter 61 / 351 | time 1[s] | loss 0.94\n",
      "| epoch 11 |  iter 81 / 351 | time 2[s] | loss 0.94\n",
      "| epoch 11 |  iter 101 / 351 | time 2[s] | loss 0.94\n",
      "| epoch 11 |  iter 121 / 351 | time 2[s] | loss 0.94\n",
      "| epoch 11 |  iter 141 / 351 | time 3[s] | loss 0.93\n",
      "| epoch 11 |  iter 161 / 351 | time 3[s] | loss 0.93\n",
      "| epoch 11 |  iter 181 / 351 | time 4[s] | loss 0.94\n",
      "| epoch 11 |  iter 201 / 351 | time 4[s] | loss 0.94\n",
      "| epoch 11 |  iter 221 / 351 | time 5[s] | loss 0.94\n",
      "| epoch 11 |  iter 241 / 351 | time 5[s] | loss 0.94\n",
      "| epoch 11 |  iter 261 / 351 | time 6[s] | loss 0.96\n",
      "| epoch 11 |  iter 281 / 351 | time 6[s] | loss 0.93\n",
      "| epoch 11 |  iter 301 / 351 | time 7[s] | loss 0.92\n",
      "| epoch 11 |  iter 321 / 351 | time 7[s] | loss 0.93\n",
      "| epoch 11 |  iter 341 / 351 | time 8[s] | loss 0.93\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 161 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1121\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 661 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 172 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 418 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 875 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1429\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 868 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 238 \n",
      "---\n",
      "val acc 6.400%\n",
      "| epoch 12 |  iter 1 / 351 | time 0[s] | loss 0.93\n",
      "| epoch 12 |  iter 21 / 351 | time 0[s] | loss 0.92\n",
      "| epoch 12 |  iter 41 / 351 | time 1[s] | loss 0.94\n",
      "| epoch 12 |  iter 61 / 351 | time 1[s] | loss 0.93\n",
      "| epoch 12 |  iter 81 / 351 | time 2[s] | loss 0.91\n",
      "| epoch 12 |  iter 101 / 351 | time 2[s] | loss 0.96\n",
      "| epoch 12 |  iter 121 / 351 | time 3[s] | loss 0.93\n",
      "| epoch 12 |  iter 141 / 351 | time 3[s] | loss 0.92\n",
      "| epoch 12 |  iter 161 / 351 | time 3[s] | loss 0.93\n",
      "| epoch 12 |  iter 181 / 351 | time 4[s] | loss 0.93\n",
      "| epoch 12 |  iter 201 / 351 | time 4[s] | loss 0.91\n",
      "| epoch 12 |  iter 221 / 351 | time 5[s] | loss 0.90\n",
      "| epoch 12 |  iter 241 / 351 | time 5[s] | loss 0.91\n",
      "| epoch 12 |  iter 261 / 351 | time 6[s] | loss 0.89\n",
      "| epoch 12 |  iter 281 / 351 | time 6[s] | loss 0.91\n",
      "| epoch 12 |  iter 301 / 351 | time 7[s] | loss 0.91\n",
      "| epoch 12 |  iter 321 / 351 | time 7[s] | loss 0.92\n",
      "| epoch 12 |  iter 341 / 351 | time 8[s] | loss 0.94\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 164 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1166\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 671 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 175 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 427 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 866 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1049\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1424\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 857 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 247 \n",
      "---\n",
      "val acc 5.340%\n",
      "| epoch 13 |  iter 1 / 351 | time 0[s] | loss 0.89\n",
      "| epoch 13 |  iter 21 / 351 | time 0[s] | loss 0.94\n",
      "| epoch 13 |  iter 41 / 351 | time 0[s] | loss 0.94\n",
      "| epoch 13 |  iter 61 / 351 | time 1[s] | loss 0.93\n",
      "| epoch 13 |  iter 81 / 351 | time 1[s] | loss 0.93\n",
      "| epoch 13 |  iter 101 / 351 | time 2[s] | loss 0.92\n",
      "| epoch 13 |  iter 121 / 351 | time 2[s] | loss 0.94\n",
      "| epoch 13 |  iter 141 / 351 | time 3[s] | loss 0.91\n",
      "| epoch 13 |  iter 161 / 351 | time 3[s] | loss 0.90\n",
      "| epoch 13 |  iter 181 / 351 | time 4[s] | loss 0.89\n",
      "| epoch 13 |  iter 201 / 351 | time 4[s] | loss 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 13 |  iter 221 / 351 | time 5[s] | loss 0.89\n",
      "| epoch 13 |  iter 241 / 351 | time 5[s] | loss 0.89\n",
      "| epoch 13 |  iter 261 / 351 | time 5[s] | loss 0.93\n",
      "| epoch 13 |  iter 281 / 351 | time 6[s] | loss 0.91\n",
      "| epoch 13 |  iter 301 / 351 | time 6[s] | loss 0.89\n",
      "| epoch 13 |  iter 321 / 351 | time 7[s] | loss 0.89\n",
      "| epoch 13 |  iter 341 / 351 | time 7[s] | loss 0.91\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 158 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1121\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 659 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[92m\u001b[0m 163 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 419 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 846 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1404\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 857 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 239 \n",
      "---\n",
      "val acc 5.040%\n",
      "| epoch 14 |  iter 1 / 351 | time 0[s] | loss 0.93\n",
      "| epoch 14 |  iter 21 / 351 | time 0[s] | loss 0.92\n",
      "| epoch 14 |  iter 41 / 351 | time 0[s] | loss 0.91\n",
      "| epoch 14 |  iter 61 / 351 | time 1[s] | loss 0.91\n",
      "| epoch 14 |  iter 81 / 351 | time 1[s] | loss 0.91\n",
      "| epoch 14 |  iter 101 / 351 | time 2[s] | loss 0.92\n",
      "| epoch 14 |  iter 121 / 351 | time 2[s] | loss 0.91\n",
      "| epoch 14 |  iter 141 / 351 | time 3[s] | loss 0.91\n",
      "| epoch 14 |  iter 161 / 351 | time 3[s] | loss 0.91\n",
      "| epoch 14 |  iter 181 / 351 | time 4[s] | loss 0.90\n",
      "| epoch 14 |  iter 201 / 351 | time 4[s] | loss 0.91\n",
      "| epoch 14 |  iter 221 / 351 | time 5[s] | loss 0.91\n",
      "| epoch 14 |  iter 241 / 351 | time 5[s] | loss 0.90\n",
      "| epoch 14 |  iter 261 / 351 | time 6[s] | loss 0.91\n",
      "| epoch 14 |  iter 281 / 351 | time 6[s] | loss 0.89\n",
      "| epoch 14 |  iter 301 / 351 | time 6[s] | loss 0.87\n",
      "| epoch 14 |  iter 321 / 351 | time 7[s] | loss 0.88\n",
      "| epoch 14 |  iter 341 / 351 | time 7[s] | loss 0.88\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 160 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1160\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 655 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 165 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 412 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 855 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1070\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1451\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 859 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 239 \n",
      "---\n",
      "val acc 6.660%\n",
      "| epoch 15 |  iter 1 / 351 | time 0[s] | loss 0.85\n",
      "| epoch 15 |  iter 21 / 351 | time 0[s] | loss 0.91\n",
      "| epoch 15 |  iter 41 / 351 | time 0[s] | loss 0.92\n",
      "| epoch 15 |  iter 61 / 351 | time 1[s] | loss 0.94\n",
      "| epoch 15 |  iter 81 / 351 | time 1[s] | loss 0.88\n",
      "| epoch 15 |  iter 101 / 351 | time 2[s] | loss 0.88\n",
      "| epoch 15 |  iter 121 / 351 | time 2[s] | loss 0.89\n",
      "| epoch 15 |  iter 141 / 351 | time 3[s] | loss 0.91\n",
      "| epoch 15 |  iter 161 / 351 | time 3[s] | loss 0.90\n",
      "| epoch 15 |  iter 181 / 351 | time 4[s] | loss 0.89\n",
      "| epoch 15 |  iter 201 / 351 | time 4[s] | loss 0.89\n",
      "| epoch 15 |  iter 221 / 351 | time 5[s] | loss 0.86\n",
      "| epoch 15 |  iter 241 / 351 | time 5[s] | loss 0.88\n",
      "| epoch 15 |  iter 261 / 351 | time 5[s] | loss 0.87\n",
      "| epoch 15 |  iter 281 / 351 | time 6[s] | loss 0.87\n",
      "| epoch 15 |  iter 301 / 351 | time 6[s] | loss 0.87\n",
      "| epoch 15 |  iter 321 / 351 | time 7[s] | loss 0.86\n",
      "| epoch 15 |  iter 341 / 351 | time 7[s] | loss 0.87\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 164 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1136\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[92m\u001b[0m 666 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 158 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 424 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 866 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1063\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1424\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 868 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 237 \n",
      "---\n",
      "val acc 6.440%\n",
      "| epoch 16 |  iter 1 / 351 | time 0[s] | loss 0.85\n",
      "| epoch 16 |  iter 21 / 351 | time 0[s] | loss 0.87\n",
      "| epoch 16 |  iter 41 / 351 | time 0[s] | loss 0.87\n",
      "| epoch 16 |  iter 61 / 351 | time 1[s] | loss 0.90\n",
      "| epoch 16 |  iter 81 / 351 | time 1[s] | loss 0.88\n",
      "| epoch 16 |  iter 101 / 351 | time 2[s] | loss 0.89\n",
      "| epoch 16 |  iter 121 / 351 | time 2[s] | loss 0.86\n",
      "| epoch 16 |  iter 141 / 351 | time 3[s] | loss 0.88\n",
      "| epoch 16 |  iter 161 / 351 | time 3[s] | loss 0.86\n",
      "| epoch 16 |  iter 181 / 351 | time 4[s] | loss 0.85\n",
      "| epoch 16 |  iter 201 / 351 | time 4[s] | loss 0.85\n",
      "| epoch 16 |  iter 221 / 351 | time 5[s] | loss 0.86\n",
      "| epoch 16 |  iter 241 / 351 | time 5[s] | loss 0.87\n",
      "| epoch 16 |  iter 261 / 351 | time 5[s] | loss 0.86\n",
      "| epoch 16 |  iter 281 / 351 | time 6[s] | loss 0.88\n",
      "| epoch 16 |  iter 301 / 351 | time 6[s] | loss 0.88\n",
      "| epoch 16 |  iter 321 / 351 | time 7[s] | loss 0.88\n",
      "| epoch 16 |  iter 341 / 351 | time 7[s] | loss 0.87\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 164 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1162\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 672 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 172 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[92m\u001b[0m 422 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 859 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1072\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1449\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 859 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 237 \n",
      "---\n",
      "val acc 6.580%\n",
      "| epoch 17 |  iter 1 / 351 | time 0[s] | loss 0.87\n",
      "| epoch 17 |  iter 21 / 351 | time 0[s] | loss 0.86\n",
      "| epoch 17 |  iter 41 / 351 | time 1[s] | loss 0.86\n",
      "| epoch 17 |  iter 61 / 351 | time 1[s] | loss 0.84\n",
      "| epoch 17 |  iter 81 / 351 | time 1[s] | loss 0.88\n",
      "| epoch 17 |  iter 101 / 351 | time 2[s] | loss 0.88\n",
      "| epoch 17 |  iter 121 / 351 | time 2[s] | loss 0.87\n",
      "| epoch 17 |  iter 141 / 351 | time 3[s] | loss 0.86\n",
      "| epoch 17 |  iter 161 / 351 | time 3[s] | loss 0.85\n",
      "| epoch 17 |  iter 181 / 351 | time 4[s] | loss 0.86\n",
      "| epoch 17 |  iter 201 / 351 | time 4[s] | loss 0.84\n",
      "| epoch 17 |  iter 221 / 351 | time 5[s] | loss 0.83\n",
      "| epoch 17 |  iter 241 / 351 | time 5[s] | loss 0.84\n",
      "| epoch 17 |  iter 261 / 351 | time 6[s] | loss 0.86\n",
      "| epoch 17 |  iter 281 / 351 | time 6[s] | loss 0.86\n",
      "| epoch 17 |  iter 301 / 351 | time 7[s] | loss 0.88\n",
      "| epoch 17 |  iter 321 / 351 | time 7[s] | loss 0.85\n",
      "| epoch 17 |  iter 341 / 351 | time 8[s] | loss 0.86\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[92m\u001b[0m 162 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1128\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 667 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 159 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 419 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 866 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1071\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1424\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 249 \n",
      "---\n",
      "val acc 7.660%\n",
      "| epoch 18 |  iter 1 / 351 | time 0[s] | loss 0.83\n",
      "| epoch 18 |  iter 21 / 351 | time 0[s] | loss 0.83\n",
      "| epoch 18 |  iter 41 / 351 | time 1[s] | loss 0.87\n",
      "| epoch 18 |  iter 61 / 351 | time 1[s] | loss 0.87\n",
      "| epoch 18 |  iter 81 / 351 | time 1[s] | loss 0.86\n",
      "| epoch 18 |  iter 101 / 351 | time 2[s] | loss 0.85\n",
      "| epoch 18 |  iter 121 / 351 | time 2[s] | loss 0.86\n",
      "| epoch 18 |  iter 141 / 351 | time 3[s] | loss 0.87\n",
      "| epoch 18 |  iter 161 / 351 | time 3[s] | loss 0.87\n",
      "| epoch 18 |  iter 181 / 351 | time 4[s] | loss 0.86\n",
      "| epoch 18 |  iter 201 / 351 | time 4[s] | loss 0.85\n",
      "| epoch 18 |  iter 221 / 351 | time 5[s] | loss 0.85\n",
      "| epoch 18 |  iter 241 / 351 | time 5[s] | loss 0.84\n",
      "| epoch 18 |  iter 261 / 351 | time 5[s] | loss 0.83\n",
      "| epoch 18 |  iter 281 / 351 | time 6[s] | loss 0.83\n",
      "| epoch 18 |  iter 301 / 351 | time 6[s] | loss 0.84\n",
      "| epoch 18 |  iter 321 / 351 | time 7[s] | loss 0.83\n",
      "| epoch 18 |  iter 341 / 351 | time 7[s] | loss 0.84\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[92m\u001b[0m 162 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1128\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 667 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 159 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 421 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 864 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1049\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1418\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 239 \n",
      "---\n",
      "val acc 9.540%\n",
      "| epoch 19 |  iter 1 / 351 | time 0[s] | loss 0.80\n",
      "| epoch 19 |  iter 21 / 351 | time 0[s] | loss 0.86\n",
      "| epoch 19 |  iter 41 / 351 | time 0[s] | loss 0.92\n",
      "| epoch 19 |  iter 61 / 351 | time 1[s] | loss 0.84\n",
      "| epoch 19 |  iter 81 / 351 | time 1[s] | loss 0.83\n",
      "| epoch 19 |  iter 101 / 351 | time 2[s] | loss 0.82\n",
      "| epoch 19 |  iter 121 / 351 | time 2[s] | loss 0.81\n",
      "| epoch 19 |  iter 141 / 351 | time 3[s] | loss 0.85\n",
      "| epoch 19 |  iter 161 / 351 | time 3[s] | loss 0.85\n",
      "| epoch 19 |  iter 181 / 351 | time 4[s] | loss 0.86\n",
      "| epoch 19 |  iter 201 / 351 | time 5[s] | loss 0.84\n",
      "| epoch 19 |  iter 221 / 351 | time 5[s] | loss 0.84\n",
      "| epoch 19 |  iter 241 / 351 | time 6[s] | loss 0.86\n",
      "| epoch 19 |  iter 261 / 351 | time 6[s] | loss 0.84\n",
      "| epoch 19 |  iter 281 / 351 | time 7[s] | loss 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 19 |  iter 301 / 351 | time 7[s] | loss 0.84\n",
      "| epoch 19 |  iter 321 / 351 | time 7[s] | loss 0.84\n",
      "| epoch 19 |  iter 341 / 351 | time 8[s] | loss 0.84\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[92m\u001b[0m 162 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1126\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 667 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 157 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 419 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 851 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1410\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 237 \n",
      "---\n",
      "val acc 7.460%\n",
      "| epoch 20 |  iter 1 / 351 | time 0[s] | loss 0.83\n",
      "| epoch 20 |  iter 21 / 351 | time 0[s] | loss 0.82\n",
      "| epoch 20 |  iter 41 / 351 | time 0[s] | loss 0.83\n",
      "| epoch 20 |  iter 61 / 351 | time 1[s] | loss 0.83\n",
      "| epoch 20 |  iter 81 / 351 | time 1[s] | loss 0.83\n",
      "| epoch 20 |  iter 101 / 351 | time 2[s] | loss 0.83\n",
      "| epoch 20 |  iter 121 / 351 | time 2[s] | loss 0.83\n",
      "| epoch 20 |  iter 141 / 351 | time 3[s] | loss 0.86\n",
      "| epoch 20 |  iter 161 / 351 | time 3[s] | loss 0.85\n",
      "| epoch 20 |  iter 181 / 351 | time 4[s] | loss 0.81\n",
      "| epoch 20 |  iter 201 / 351 | time 4[s] | loss 0.82\n",
      "| epoch 20 |  iter 221 / 351 | time 4[s] | loss 0.83\n",
      "| epoch 20 |  iter 241 / 351 | time 5[s] | loss 0.81\n",
      "| epoch 20 |  iter 261 / 351 | time 5[s] | loss 0.79\n",
      "| epoch 20 |  iter 281 / 351 | time 6[s] | loss 0.80\n",
      "| epoch 20 |  iter 301 / 351 | time 6[s] | loss 0.82\n",
      "| epoch 20 |  iter 321 / 351 | time 7[s] | loss 0.93\n",
      "| epoch 20 |  iter 341 / 351 | time 7[s] | loss 0.83\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[92m\u001b[0m 162 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1131\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 667 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 164 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 419 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 862 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1049\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1424\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 871 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 239 \n",
      "---\n",
      "val acc 10.240%\n",
      "| epoch 21 |  iter 1 / 351 | time 0[s] | loss 0.79\n",
      "| epoch 21 |  iter 21 / 351 | time 0[s] | loss 0.80\n",
      "| epoch 21 |  iter 41 / 351 | time 0[s] | loss 0.81\n",
      "| epoch 21 |  iter 61 / 351 | time 1[s] | loss 0.81\n",
      "| epoch 21 |  iter 81 / 351 | time 1[s] | loss 0.82\n",
      "| epoch 21 |  iter 101 / 351 | time 2[s] | loss 0.80\n",
      "| epoch 21 |  iter 121 / 351 | time 2[s] | loss 0.82\n",
      "| epoch 21 |  iter 141 / 351 | time 3[s] | loss 0.86\n",
      "| epoch 21 |  iter 161 / 351 | time 3[s] | loss 0.81\n",
      "| epoch 21 |  iter 181 / 351 | time 4[s] | loss 0.80\n",
      "| epoch 21 |  iter 201 / 351 | time 4[s] | loss 0.82\n",
      "| epoch 21 |  iter 221 / 351 | time 4[s] | loss 0.82\n",
      "| epoch 21 |  iter 241 / 351 | time 5[s] | loss 0.82\n",
      "| epoch 21 |  iter 261 / 351 | time 5[s] | loss 0.81\n",
      "| epoch 21 |  iter 281 / 351 | time 6[s] | loss 0.83\n",
      "| epoch 21 |  iter 301 / 351 | time 6[s] | loss 0.81\n",
      "| epoch 21 |  iter 321 / 351 | time 7[s] | loss 0.80\n",
      "| epoch 21 |  iter 341 / 351 | time 7[s] | loss 0.87\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[92m\u001b[0m 162 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1138\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[92m\u001b[0m 666 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 166 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 420 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 856 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1063\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1424\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 237 \n",
      "---\n",
      "val acc 9.420%\n",
      "| epoch 22 |  iter 1 / 351 | time 0[s] | loss 0.81\n",
      "| epoch 22 |  iter 21 / 351 | time 0[s] | loss 0.78\n",
      "| epoch 22 |  iter 41 / 351 | time 0[s] | loss 0.79\n",
      "| epoch 22 |  iter 61 / 351 | time 1[s] | loss 0.80\n",
      "| epoch 22 |  iter 81 / 351 | time 1[s] | loss 0.78\n",
      "| epoch 22 |  iter 101 / 351 | time 2[s] | loss 0.83\n",
      "| epoch 22 |  iter 121 / 351 | time 2[s] | loss 0.82\n",
      "| epoch 22 |  iter 141 / 351 | time 3[s] | loss 0.82\n",
      "| epoch 22 |  iter 161 / 351 | time 3[s] | loss 0.83\n",
      "| epoch 22 |  iter 181 / 351 | time 4[s] | loss 0.82\n",
      "| epoch 22 |  iter 201 / 351 | time 4[s] | loss 0.82\n",
      "| epoch 22 |  iter 221 / 351 | time 5[s] | loss 0.83\n",
      "| epoch 22 |  iter 241 / 351 | time 5[s] | loss 0.82\n",
      "| epoch 22 |  iter 261 / 351 | time 5[s] | loss 0.81\n",
      "| epoch 22 |  iter 281 / 351 | time 6[s] | loss 0.81\n",
      "| epoch 22 |  iter 301 / 351 | time 6[s] | loss 0.80\n",
      "| epoch 22 |  iter 321 / 351 | time 7[s] | loss 0.81\n",
      "| epoch 22 |  iter 341 / 351 | time 7[s] | loss 0.82\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 163 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1137\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 667 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 162 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[92m\u001b[0m 422 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 856 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1062\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1420\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 242 \n",
      "---\n",
      "val acc 9.040%\n",
      "| epoch 23 |  iter 1 / 351 | time 0[s] | loss 0.76\n",
      "| epoch 23 |  iter 21 / 351 | time 0[s] | loss 0.77\n",
      "| epoch 23 |  iter 41 / 351 | time 0[s] | loss 0.81\n",
      "| epoch 23 |  iter 61 / 351 | time 1[s] | loss 0.82\n",
      "| epoch 23 |  iter 81 / 351 | time 1[s] | loss 0.81\n",
      "| epoch 23 |  iter 101 / 351 | time 2[s] | loss 0.78\n",
      "| epoch 23 |  iter 121 / 351 | time 2[s] | loss 0.77\n",
      "| epoch 23 |  iter 141 / 351 | time 3[s] | loss 0.79\n",
      "| epoch 23 |  iter 161 / 351 | time 3[s] | loss 0.81\n",
      "| epoch 23 |  iter 181 / 351 | time 4[s] | loss 0.81\n",
      "| epoch 23 |  iter 201 / 351 | time 4[s] | loss 0.79\n",
      "| epoch 23 |  iter 221 / 351 | time 4[s] | loss 0.83\n",
      "| epoch 23 |  iter 241 / 351 | time 5[s] | loss 0.82\n",
      "| epoch 23 |  iter 261 / 351 | time 5[s] | loss 0.80\n",
      "| epoch 23 |  iter 281 / 351 | time 6[s] | loss 0.78\n",
      "| epoch 23 |  iter 301 / 351 | time 6[s] | loss 0.79\n",
      "| epoch 23 |  iter 321 / 351 | time 7[s] | loss 0.77\n",
      "| epoch 23 |  iter 341 / 351 | time 7[s] | loss 0.79\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 158 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1125\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[92m\u001b[0m 666 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 160 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 412 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 850 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1415\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 862 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 232 \n",
      "---\n",
      "val acc 6.860%\n",
      "| epoch 24 |  iter 1 / 351 | time 0[s] | loss 0.82\n",
      "| epoch 24 |  iter 21 / 351 | time 0[s] | loss 0.82\n",
      "| epoch 24 |  iter 41 / 351 | time 0[s] | loss 0.82\n",
      "| epoch 24 |  iter 61 / 351 | time 1[s] | loss 0.81\n",
      "| epoch 24 |  iter 81 / 351 | time 1[s] | loss 0.77\n",
      "| epoch 24 |  iter 101 / 351 | time 2[s] | loss 0.77\n",
      "| epoch 24 |  iter 121 / 351 | time 2[s] | loss 0.79\n",
      "| epoch 24 |  iter 141 / 351 | time 3[s] | loss 0.81\n",
      "| epoch 24 |  iter 161 / 351 | time 3[s] | loss 0.79\n",
      "| epoch 24 |  iter 181 / 351 | time 4[s] | loss 0.78\n",
      "| epoch 24 |  iter 201 / 351 | time 4[s] | loss 0.78\n",
      "| epoch 24 |  iter 221 / 351 | time 5[s] | loss 0.80\n",
      "| epoch 24 |  iter 241 / 351 | time 5[s] | loss 0.80\n",
      "| epoch 24 |  iter 261 / 351 | time 5[s] | loss 0.81\n",
      "| epoch 24 |  iter 281 / 351 | time 6[s] | loss 0.79\n",
      "| epoch 24 |  iter 301 / 351 | time 6[s] | loss 0.79\n",
      "| epoch 24 |  iter 321 / 351 | time 7[s] | loss 0.79\n",
      "| epoch 24 |  iter 341 / 351 | time 7[s] | loss 0.81\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 161 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m\u001b[0m 1162\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 671 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 165 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 427 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 855 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m\u001b[0m 1061\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1441\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 861 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 235 \n",
      "---\n",
      "val acc 8.940%\n",
      "| epoch 25 |  iter 1 / 351 | time 0[s] | loss 0.81\n",
      "| epoch 25 |  iter 21 / 351 | time 0[s] | loss 0.77\n",
      "| epoch 25 |  iter 41 / 351 | time 1[s] | loss 0.77\n",
      "| epoch 25 |  iter 61 / 351 | time 1[s] | loss 0.77\n",
      "| epoch 25 |  iter 81 / 351 | time 2[s] | loss 0.76\n",
      "| epoch 25 |  iter 101 / 351 | time 2[s] | loss 0.75\n",
      "| epoch 25 |  iter 121 / 351 | time 3[s] | loss 0.77\n",
      "| epoch 25 |  iter 141 / 351 | time 3[s] | loss 0.77\n",
      "| epoch 25 |  iter 161 / 351 | time 4[s] | loss 0.76\n",
      "| epoch 25 |  iter 181 / 351 | time 4[s] | loss 0.76\n",
      "| epoch 25 |  iter 201 / 351 | time 5[s] | loss 0.78\n",
      "| epoch 25 |  iter 221 / 351 | time 5[s] | loss 0.76\n",
      "| epoch 25 |  iter 241 / 351 | time 6[s] | loss 0.77\n",
      "| epoch 25 |  iter 261 / 351 | time 6[s] | loss 0.79\n",
      "| epoch 25 |  iter 281 / 351 | time 7[s] | loss 0.76\n",
      "| epoch 25 |  iter 301 / 351 | time 7[s] | loss 0.79\n",
      "| epoch 25 |  iter 321 / 351 | time 8[s] | loss 0.86\n",
      "| epoch 25 |  iter 341 / 351 | time 8[s] | loss 0.79\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m\u001b[0m 161 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[92m\u001b[0m 1139\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m\u001b[0m 667 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m\u001b[0m 162 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m\u001b[0m 424 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m\u001b[0m 856 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[92m\u001b[0m 1053\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m\u001b[0m 1418\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m\u001b[0m 861 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m\u001b[0m 237 \n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 12.300%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "\n",
    "# \n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# Reverse input? =================================================\n",
    "is_reverse = False  # True\n",
    "if is_reverse:\n",
    "    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "# ================================================================\n",
    "\n",
    "# \n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "\n",
    "# Normal or Peeky? ==============================================\n",
    "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# ================================================================\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "# \n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
